{
  "papers": [
    {
      "type": "research-article",
      "title": "Shared memory protection in a multi-tenant JVM",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480714",
      "doi": "10.1145/3475738.3480714",
      "authors": [
        "Majid Makki",
        "Dimitri Van Landuyt",
        "Bert Lagaisse",
        "Wouter Joosen"
      ],
      "monthYear": "September 2021",
      "pages": "pp 1–15",
      "shortAbstract": "<p>Multi-tenant Software-as-a-Service (SaaS) providers allow tenants to customize the application at different levels. When the customization involves tenant custom code and a single application instance is shared among multiple tenants, the issue of ...</p><p></p><p></p><p></p>",
      "citations": 0,
      "downloads": 75,
      "id": 0,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>Multi-tenant Software-as-a-Service (SaaS) providers allow tenants to customize the application at different levels. When the customization involves tenant custom code and a single application instance is shared among multiple tenants, the issue of tenant isolation becomes critical. In common practice, tenant isolation, which amounts to protection of tenants against any interference and disturbance from each other, is performed by isolating tenant custom code in either a dedicated Virtual Machine (VM) or a dedicated container. </p> <p> However, tenant isolation can be enforced at the higher level of threads rather than OS processes. The main advantage of this alternative approach is that it significantly increases tenant accommodation capacity (the number of tenants that can be hosted on a single node). However, achieving this benefit raises a number of non-trivial challenges, most notably the need for access control over the memory space shared between the custom code of multiple tenants. </p> <p> In this paper, we present a solution for protecting the shared memory space of the Java Virtual Machine (JVM) demarcated by the static fields of the java.base module. The solution is based on systematic analysis of the java.base module. As a result of the analysis, the set of shared classes is reduced to a minimal subset of the java.base module and it is shown that the static fields of the minimal subset can be protected using the Java platform security. A multi-tenant class loading mechanism is also provided for loading a tenant-specific runtime instance of classes not included in the minimal subset. </p> <p> The proposed solution is implemented on top of a customized OpenJDK 11 and validated by means of 18 validation scenarios. The evaluation results presented in this paper show that achieving a memory footprint reduction ranging between 32% and 97% requires only 32 CLOC in OpenJDK source and denial of only 9 distinct permissions for tenants without any significant performance overhead for a wide range of application domains.</p>\n\t"
    },
    {
      "type": "short-paper",
      "title": "Cross-ISA testing of the Pharo VM: lessons learned while porting to ARMv8",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480715",
      "doi": "10.1145/3475738.3480715",
      "authors": [
        "Guillermo Polito",
        "Pablo Tesone",
        "Stéphane Ducasse",
        "Luc Fabresse",
        "Théo Rogliano",
        "Pierre Misse-Chanabier",
        "Carolina Hernandez Phillips"
      ],
      "monthYear": "September 2021",
      "pages": "pp 16–25",
      "shortAbstract": "<p>Testing and debugging a Virtual Machine is a laborious task without the proper tooling. This is particularly true for VMs with JIT compilation and dynamic code patching for techniques such as inline caching. In addition, this situation is getting worse ...</p><p></p><p></p>",
      "citations": 2,
      "downloads": 41,
      "id": 1,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>Testing and debugging a Virtual Machine is a laborious task without the proper tooling. This is particularly true for VMs with JIT compilation and dynamic code patching for techniques such as inline caching. In addition, this situation is getting worse when the VM builds and runs on multiple target architectures. </p> <p> In this paper, we report on several lessons we learned while testing the Pharo VM, particularly during the port of its Cogit JIT compiler to the AArch64 architecture. The Pharo VM presented already a simulation environment that is very handy to simulate full executions and live-develop the VM. However, this full simulation environment makes it difficult to reproduce short and simple testing scenarios. We extended the pre-existing simulation environment with a testing infrastructure and a methodology that allow us to have fine-grained control of testing scenarios, making tests small, fast, reproducible, and cross-ISA. </p> <p> We report on how this testing infrastructure allowed us to cope with two different development scenarios: (1) porting the Cogit JIT compiler to AArch64 without early access to real hardware and (2) debugging memory corruptions due to GC bugs.</p>\n\t"
    },
    {
      "type": "short-paper",
      "title": "Higher-order concurrency for microcontrollers",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480716",
      "doi": "10.1145/3475738.3480716",
      "authors": [
        "Abhiroop Sarkar",
        "Robert Krook",
        "Bo Joel Svensson",
        "Mary Sheeran"
      ],
      "monthYear": "September 2021",
      "pages": "pp 26–35",
      "shortAbstract": "<p>Programming microcontrollers involves low level interfacing with hardware and peripherals that are concurrent and reactive. Such programs are typically written in a mixture of C and assembly using concurrent language extensions (like FreeRTOS tasks and ...</p><p><em></em></p><p><em></em></p>",
      "citations": 0,
      "downloads": 60,
      "id": 2,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>Programming microcontrollers involves low level interfacing with hardware and peripherals that are concurrent and reactive. Such programs are typically written in a mixture of C and assembly using concurrent language extensions (like FreeRTOS tasks and semaphores), resulting in unsafe, callback-driven, error-prone and difficult-to-maintain code. </p> <p>We address this challenge by introducing SenseVM - a bytecode-interpreted virtual machine that provides a message passing based <i>higher-order concurrency</i> model, originally introduced by Reppy, for microcontroller programming. This model treats synchronous operations as first-class values (called Events) akin to the treatment of first-class functions in functional languages. This primarily allows the programmer to compose and tailor their own concurrency abstractions and, additionally, abstracts away unsafe memory operations, common in shared-memory concurrency models, thereby making microcontroller programs safer, composable and easier-to-maintain. </p> <p>Our VM is made portable via a low-level <i>bridge</i> interface, built atop the embedded OS - Zephyr. The bridge is implemented by all drivers and designed such that programming in response to a software message or a hardware interrupt remains uniform and indistinguishable. In this paper we demonstrate the features of our VM through an example, written in a Caml-like functional language, running on the nRF52840 and STM32F4 microcontrollers.</p>\n\t"
    },
    {
      "type": "short-paper",
      "title": "Virtual ADTs for portable metaprogramming",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480717",
      "doi": "10.1145/3475738.3480717",
      "authors": [
        "Nicolas Stucki",
        "Jonathan Immanuel Brachthäuser",
        "Martin Odersky"
      ],
      "monthYear": "September 2021",
      "pages": "pp 36–44",
      "shortAbstract": "<p>Scala 3 provides a metaprogramming interface that represents the <em>abstract syntax tree</em> definitions using <em>algebraic data types</em>. To allow the compiler to freely evolve without breaking the metaprogramming interface, we present <em>virtual algebraic data types</em> (...</p>",
      "citations": 0,
      "downloads": 70,
      "id": 3,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>Scala 3 provides a metaprogramming interface that represents the <i>abstract syntax tree</i> definitions using <i>algebraic data types</i>. To allow the compiler to freely evolve without breaking the metaprogramming interface, we present <i>virtual algebraic data types</i> (or Virtual ADTs) -- a programming pattern, which allows programmers to describe mutually recursive hierarchies of types without coupling to a particular runtime representation.</p>\n\t"
    },
    {
      "type": "short-paper",
      "title": "Specializing generic Java data structures",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480718",
      "doi": "10.1145/3475738.3480718",
      "authors": [
        "Dan Graur",
        "Rodrigo Bruno",
        "Gustavo Alonso"
      ],
      "monthYear": "September 2021",
      "pages": "pp 45–53",
      "shortAbstract": "<p>The Collections framework is an essential utility in virtually every Java application. It offers a set of fundamental data structures that exploit Java Generics and the Object type in order to enable a high degree of reusability. Upon instantiation, ...</p><p></p>",
      "citations": 0,
      "downloads": 102,
      "id": 4,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>The Collections framework is an essential utility in virtually every Java application. It offers a set of fundamental data structures that exploit Java Generics and the Object type in order to enable a high degree of reusability. Upon instantiation, Collections are parametrized by the type they are meant to store. However, at compile-time, due to type erasure, this type gets replaced by Object, forcing the data structures to manipulate references of type Object (the root of the Java type system). In the bytecode, the compiler transparently adds type checking instructions to ensure type safety, and generates bridge methods to enable the polymorphic behavior of parametrized classes. This approach can introduce non-trivial runtime overheads when applications extensively manipulate Collections. </p> <p> We propose the Java Collections Specializer (JCS), a tool we have developed to deliver truly specialized Collections. JCS can generate ArrayLists, ConcurrentHashMaps and HashMaps with true type specialization that incur no performance penalties due to bridge methods or type checking instructions. JCS offers the possibility to easily extend its use to other Collection data structures. Since the specialized data structures extend and inherit from the generic counterpart's superclasses and interfaces, the specialized versions can be used in most places where generic versions are employed. The programmer uses JCS to generate specializations ahead of time. These are generated under the java.util package, and need only be added to the class path and integrated into the application logic. We show that the specialized data structures can improve the runtime performance of data intensive workloads by up to 14% for read use-cases and 42% for write use-cases.</p>\n\t"
    },
    {
      "type": "short-paper",
      "title": "Architecture-agnostic dynamic type recovery",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480719",
      "doi": "10.1145/3475738.3480719",
      "authors": [
        "Daniel Pekarek",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2021",
      "pages": "pp 54–60",
      "shortAbstract": "<p>Programmers can use various data types when developing software. However, if the program is compiled to machine code, most of this type information is lost. If analysis of a compiled program is necessary, the lost data types have to be recovered again, ...</p><p></p>",
      "citations": 0,
      "downloads": 58,
      "id": 5,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>Programmers can use various data types when developing software. However, if the program is compiled to machine code, most of this type information is lost. If analysis of a compiled program is necessary, the lost data types have to be recovered again, to make the code understandable. Existing approaches for the type recovery problem require detailed knowledge about the CPU architecture in question. An architecture-agnostic approach is missing so far. </p> <p> This work focuses on a truly architecture-agnostic type recovery algorithm, implemented in a dynamic analysis system. It can recover data types using minimal knowledge about the CPU architecture, therefore, making it easy to support many different CPU architectures in the analysis system.</p>\n\t"
    },
    {
      "type": "short-paper",
      "title": "Profiling code cache behaviour via events",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480720",
      "doi": "10.1145/3475738.3480720",
      "authors": [
        "Pablo Tesone",
        "Guillermo Polito",
        "Stéphane Ducasse"
      ],
      "monthYear": "September 2021",
      "pages": "pp 61–69",
      "shortAbstract": "<p>Virtual machine performance tuning for a given application is an arduous and challenging task. For example, parametrizing the behaviour of the JIT compiler machine code caches affects the overall performance of applications while being rather obscure ...</p><p></p>",
      "citations": 0,
      "downloads": 32,
      "id": 6,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>Virtual machine performance tuning for a given application is an arduous and challenging task. For example, parametrizing the behaviour of the JIT compiler machine code caches affects the overall performance of applications while being rather obscure for final users not knowledgeable about VM internals. Moreover, VM components are often heavily coupled and changes in some parameters may affect several seemingly unrelated components and may have unclear performance impacts. Therefore, choosing the best parametrization requires to have precise information. </p> <p> In this paper, we present Vicoca, a tool that allows VM users and developers to obtain detailed information about the behaviour of the code caches and their interactions with other virtual machine components. We present a complex optimization problem due to the heavy interaction of components in the Pharo VM, and we explain it using Vicoca. The information produced by the tool allows developers to produce an optimized configuration for the VM. Vicoca is based on event recording that are manipulated during off-line analysis. Vicoca not only allows us to understand this given problem, but it opens the door to future work such as automatic detection of application characteristics, identification of performance issues, and automatic hinting.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Low-overhead multi-language dynamic taint analysis on managed runtimes through speculative optimization",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480939",
      "doi": "10.1145/3475738.3480939",
      "authors": [
        "Jacob Kreindl",
        "Daniele Bonetta",
        "Lukas Stadler",
        "David Leopoldseder",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2021",
      "pages": "pp 70–87",
      "shortAbstract": "<p><em>Dynamic taint analysis</em> (DTA) is a popular program analysis technique with applications to diverse fields such as software vulnerability detection and reverse engineering. It consists of marking sensitive data as <em>tainted</em> and tracking its propagation at ...</p><p><em></em><em></em><em></em></p>",
      "citations": 0,
      "downloads": 95,
      "id": 7,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p><i>Dynamic taint analysis</i> (DTA) is a popular program analysis technique with applications to diverse fields such as software vulnerability detection and reverse engineering. It consists of marking sensitive data as <i>tainted</i> and tracking its propagation at runtime. While DTA has been implemented on top of many different analysis platforms, these implementations generally incur significant slowdown from taint propagation. Since a purely dynamic analysis cannot predict which instructions will operate on tainted values at runtime, programs have to be fully instrumented for taint propagation even when they never actually observe tainted values. We propose leveraging speculative optimizations to reduce slowdown on the peak performance of programs instrumented for DTA on a managed runtime capable of dynamic compilation. </p> <p>In this paper, we investigate how speculative optimizations can reduce the peak performance impact of taint propagation on programs executed on a managed runtime. We also explain how a managed runtime can implement DTA to be amenable to such optimizations. We implemented our ideas in <i>TruffleTaint</i>, a DTA platform which supports both dynamic languages like JavaScript and languages like C and C++ which are typically compiled statically. We evaluated TruffleTaint on several benchmarks from the popular <i>Computer Language Benchmarks Game</i> and <i>SPECint 2017</i> benchmark suites. Our evaluation shows that TruffleTaint is often able to avoid slowdown entirely when programs do not operate on tainted data, and that it exhibits slowdown of on average ∼2.10x and up to ∼5.52x when they do, which is comparable to state-of-the-art taint analysis platforms optimized for performance.</p>\n\t"
    },
    {
      "type": "short-paper",
      "title": "Tracing and its observer effect on concurrency",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480940",
      "doi": "10.1145/3475738.3480940",
      "authors": [
        "Indigo Orton",
        "Alan Mycroft"
      ],
      "monthYear": "September 2021",
      "pages": "pp 88–96",
      "shortAbstract": "<p>Execution tracing has an observer effect: the act of tracing perturbs program behaviour via its overhead, which can in turn affect the accuracy of subsequent dynamic analysis. We investigate this observer effect in the context of concurrent behaviour ...</p>",
      "citations": 0,
      "downloads": 118,
      "id": 8,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>Execution tracing has an observer effect: the act of tracing perturbs program behaviour via its overhead, which can in turn affect the accuracy of subsequent dynamic analysis. We investigate this observer effect in the context of concurrent behaviour within JVM-based programs. Concurrent behaviour is especially fragile as task-scheduling ordering can change, which could even lead to deadlock via thread starvation under certain conditions. We analyse three dimensions of overhead, compute volume, memory volume, and uniformity, using a configurable-overhead tracer and a concurrency-performance analyser. We argue that uniformity is a key, and underappreciated, dimension of overhead that can have qualitative effects on program behaviour. Experimental results show that overhead significantly affects real-world concurrent behaviour and subsequent analysis, at times unintuitively.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Generation of TypeScript declaration files from JavaScript code",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480941",
      "doi": "10.1145/3475738.3480941",
      "authors": [
        "Fernando Cristiani",
        "Peter Thiemann"
      ],
      "monthYear": "September 2021",
      "pages": "pp 97–112",
      "shortAbstract": "<p>Developers are starting to write large and complex applications in TypeScript, a typed dialect of JavaScript. TypeScript applications integrate JavaScript libraries via typed descriptions of their APIs called declaration files. DefinitelyTyped is the ...</p><p></p>",
      "citations": 0,
      "downloads": 72,
      "id": 9,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>Developers are starting to write large and complex applications in TypeScript, a typed dialect of JavaScript. TypeScript applications integrate JavaScript libraries via typed descriptions of their APIs called declaration files. DefinitelyTyped is the standard public repository for these files. The repository is populated and maintained manually by volunteers, which is error-prone and time consuming. Discrepancies between a declaration file and the JavaScript implementation lead to incorrect feedback from the TypeScript IDE and, thus, to incorrect uses of the underlying JavaScript library.</p> <p>This work presents dts-generate, a tool that generates TypeScript declaration files for JavaScript libraries uploaded to the NPM registry. It extracts code examples from the documentation written by the developer, executes the library driven by the examples, gathers run-time information, and generates a declaration file based on this information. To evaluate the tool, 249 declaration files were generated directly from an NPM module and 111 of these were compared with the corresponding declaration file provided on DefinitelyTyped. All these files either exhibited no differences at all or differences that can be resolved by extending the developer-provided examples.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "LLJava live at the loop: a case for heteroiconic staged meta-programming",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480942",
      "doi": "10.1145/3475738.3480942",
      "authors": [
        "Baltasar Trancón y Widemann",
        "Markus Lepper"
      ],
      "monthYear": "September 2021",
      "pages": "pp 113–126",
      "shortAbstract": "<p>This paper investigates the use of staged meta-programming techniques for the transparent acceleration of embedded domain-specific languages on the Java platform. LLJava-live, the staged API of the low-level JVM language LLJava, can be used to ...</p>",
      "citations": 0,
      "downloads": 35,
      "id": 10,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>This paper investigates the use of staged meta-programming techniques for the transparent acceleration of embedded domain-specific languages on the Java platform. LLJava-live, the staged API of the low-level JVM language LLJava, can be used to complement an interpreted EDSL with orthogonal and extensible compilation facilities. Compiled JVM bytecode becomes available immediately as an extension of the running host program. The approach is illustrated with a didactic structured imperative programming language, Whilst.</p>\n\t"
    },
    {
      "type": "short-paper",
      "title": "Using machine learning to predict the code size impact of duplication heuristics in a dynamic compiler",
      "url": "https://dl.acm.org/doi/10.1145/3475738.3480943",
      "doi": "10.1145/3475738.3480943",
      "authors": [
        "Raphael Mosaner",
        "David Leopoldseder",
        "Lukas Stadler",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2021",
      "pages": "pp 127–135",
      "shortAbstract": "<p>Code duplication is a major opportunity to enable optimizations in subsequent compiler phases. However, duplicating code prematurely or too liberally can result in tremendous code size increases. Thus, modern compilers use trade-offs between estimated ...</p>",
      "citations": 0,
      "downloads": 90,
      "id": 11,
      "proceedingsId": 0,
      "fullAbstract": "\n\t\t<p>Code duplication is a major opportunity to enable optimizations in subsequent compiler phases. However, duplicating code prematurely or too liberally can result in tremendous code size increases. Thus, modern compilers use trade-offs between estimated costs in terms of code size increase and benefits in terms of performance increase. In the context of this ongoing research project, we propose the use of machine learning to provide trade-off functions with accurate predictions for code size impact. To evaluate our approach, we implemented a neural network predictor in the GraalVM compiler and compared its performance against a human-crafted, highly tuned heuristic. First results show promising performance improvements, leading to code size reductions of more than 10% for several benchmarks. Additionally, we present an assistance mode for finding flaws in the human-crafted heuristic, leading to improvements for the duplication optimization itself.</p>\n\t"
    },
    {
      "type": "abstract",
      "title": "Garbage collection: implementation, innovation, performance, and security (keynote)",
      "url": "https://dl.acm.org/doi/10.1145/3426182.3431579",
      "doi": "10.1145/3426182.3431579",
      "authors": [
        "Stephen M. Blackburn"
      ],
      "monthYear": "November 2020",
      "pages": "pp 1",
      "shortAbstract": "<p>Garbage collection is a key technology underpinning many popular languages today. However, garbage collectors are the epitome of performance- and security-critical low-level systems programming. They are notoriously difficult to implement correctly and ...</p>",
      "citations": 0,
      "downloads": 57,
      "id": 12,
      "proceedingsId": 1,
      "fullAbstract": "\n\t\t<p>Garbage collection is a key technology underpinning many popular languages today. However, garbage collectors are the epitome of performance- and security-critical low-level systems programming. They are notoriously difficult to implement correctly and are extremely performance-sensitive. Performance, innovation, and security are often viewed as being in tension; all the more so in systems programming. In this talk, I will reflect on my experience in academia and industry building garbage collectors, and will particularly address the question of how critical systems software can be engineered to maximize performance, innovation and security.</p>\n\t"
    },
    {
      "type": "abstract",
      "title": "Hardware support for managed languages: an old idea whose time has finally come? (keynote)",
      "url": "https://dl.acm.org/doi/10.1145/3426182.3431580",
      "doi": "10.1145/3426182.3431580",
      "authors": [
        "Martin Maas"
      ],
      "monthYear": "November 2020",
      "pages": "pp 2",
      "shortAbstract": "<p>A large number of workloads are written in managed languages, including server workloads in data centers, web applications in browsers, and client workloads on desktops or mobile devices. Due to their widespread adoption, improving the performance and ...</p><p></p>",
      "citations": 0,
      "downloads": 43,
      "id": 13,
      "proceedingsId": 1,
      "fullAbstract": "\n\t\t<p>A large number of workloads are written in managed languages, including server workloads in data centers, web applications in browsers, and client workloads on desktops or mobile devices. Due to their widespread adoption, improving the performance and efficiency of managed-language features such as garbage collection, JIT compilation, and dynamic runtime checks can have a significant impact on many real workloads. Hardware support for such features has been investigated since the 1970s, but still has not seen widespread practical adoption. </p> <p> In this talk, I will discuss trends that make today a great time to revisit these ideas. I will describe work done at UC Berkeley that moves garbage collection into a small hardware accelerator close to the memory controller and performs GC more efficiently than a CPU. I will also talk about current work within the open-source RISC-V project on developing standard extensions for managed-language support in the context of the free and open RISC-V ISA. Finally, I will lay out opportunities for research in this area, and how open-source infrastructure can be used to build end-to-end prototypes of this work in an academic setting.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "From causality to stability: understanding and reducing meta-data in CRDTs",
      "url": "https://dl.acm.org/doi/10.1145/3426182.3426183",
      "doi": "10.1145/3426182.3426183",
      "authors": [
        "Jim Bauwens",
        "Elisa Gonzalez Boix"
      ],
      "monthYear": "November 2020",
      "pages": "pp 3–14",
      "shortAbstract": "<p>Modern distributed applications increasingly replicate data to guarantee both high availability of systems and optimal user experience. Conflict-Free Replicated Data Types (CRDTs) are a family of data types specially designed for highly available ...</p><p></p><p></p>",
      "citations": 1,
      "downloads": 47,
      "id": 14,
      "proceedingsId": 1,
      "fullAbstract": "\n\t\t<p>Modern distributed applications increasingly replicate data to guarantee both high availability of systems and optimal user experience. Conflict-Free Replicated Data Types (CRDTs) are a family of data types specially designed for highly available systems that guarantee some form of eventual consistency. To ensure state convergence between replicas, CRDT implementations need to keep track of additional meta-data. This is not a scalable strategy, as a growing amount of meta-data has to be kept. </p> <p> In this paper, we show that existing solutions for this problem miss optimisation opportunities and may lead to less reactive CRDTs. For this, we analyse the relation between meta-data and the causality of operations in operation-based CRDTs. We explore a new optimisation strategy for pure operation-based CRDTs and show how it reduces memory overhead. Our approach takes advantage of the communication layer providing reliable delivery to determine causal stability, and as a result, meta-data can be removed sooner. We furthermore propose a solution for improving the reactivity of CRDTs built on a reliable causal broadcasting layer. </p> <p> We apply our strategy to pure-operation based CRDTs and validate our approach by measuring its impact on several different set-ups. The results show how our approach can lead to significant improvements in meta-data cleanup when compared to state-of-the-art techniques.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Multi-language dynamic taint analysis in a polyglot virtual machine",
      "url": "https://dl.acm.org/doi/10.1145/3426182.3426184",
      "doi": "10.1145/3426182.3426184",
      "authors": [
        "Jacob Kreindl",
        "Daniele Bonetta",
        "Lukas Stadler",
        "David Leopoldseder",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "November 2020",
      "pages": "pp 15–29",
      "shortAbstract": "<p><em>Dynamic taint analysis</em> is a popular program analysis technique in which sensitive data is marked as <em>tainted</em> and the propagation of tainted data is tracked in order to determine whether that data reaches critical program locations. This analysis ...</p><p><em></em><em></em></p>",
      "citations": 1,
      "downloads": 133,
      "id": 15,
      "proceedingsId": 1,
      "fullAbstract": "\n\t\t<p><i>Dynamic taint analysis</i> is a popular program analysis technique in which sensitive data is marked as <i>tainted</i> and the propagation of tainted data is tracked in order to determine whether that data reaches critical program locations. This analysis technique has been successfully applied to software vulnerability detection, malware analysis, testing and debugging, and many other fields. However, existing approaches of dynamic taint analysis are either language-specific or they target native code. Neither is suitable for analyzing applications in which high-level dynamic languages such as JavaScript and low-level languages such as C interact.In these approaches, the language boundary forms an opaque barrier that prevents a sound analysis of data flow in the other language and can thus lead to the analysis being evaded. </p> <p>In this paper we introduce <i>TruffleTaint</i>, a platform for multi-language dynamic taint analysis that uses language-independent techniques for propagating taint labels to overcome the language boundary but still allows for language-specific taint propagation rules. Based on the <i>Truffle</i> framework for implementing runtimes for programming languages, TruffleTaint supports propagating taint in and between a selection of dynamic and low-level programming languages and can be easily extended to support additional languages. We demonstrate TruffleTaint’s propagation capabilities and evaluate its performance using several benchmarks from the Computer Language Benchmarks Game, which we implemented as combinations of C, JavaScript and Python code and which we adapted to propagate taint in various scenarios of language interaction. Our evaluation shows that TruffleTaint causes low to zero slowdown when no taint is introduced, rivaling state-of-the-art dynamic taint analysis platforms, and only up to ∼40x slowdown when taint is introduced.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Efficient, near complete, and often sound hybrid dynamic data race prediction",
      "url": "https://dl.acm.org/doi/10.1145/3426182.3426185",
      "doi": "10.1145/3426182.3426185",
      "authors": [
        "Martin Sulzmann",
        "Kai Stadtmüller"
      ],
      "monthYear": "November 2020",
      "pages": "pp 30–51",
      "shortAbstract": "<p>Dynamic data race prediction aims to identify races based on a single program run represented by a trace. The challenge is to remain efficient while being as sound and as complete as possible. Efficient means a linear run-time as otherwise the method ...</p>",
      "citations": 1,
      "downloads": 34,
      "id": 16,
      "proceedingsId": 1,
      "fullAbstract": "\n\t\t<p>Dynamic data race prediction aims to identify races based on a single program run represented by a trace. The challenge is to remain efficient while being as sound and as complete as possible. Efficient means a linear run-time as otherwise the method unlikely scales for real-world programs. We introduce an efficient, near complete and often sound dynamic data race prediction method that combines the lockset method with several improvements made in the area of happens-before methods. By near complete we mean that the method is complete in theory but for efficiency reasons the implementation applies some optimizations that may result in incompleteness. The method can be shown to be sound for two threads but is unsound in general. Experiments show that our method works well in practice.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Efficient dispatch of multi-object polymorphic call sites in contextual role-oriented programming languages",
      "url": "https://dl.acm.org/doi/10.1145/3426182.3426186",
      "doi": "10.1145/3426182.3426186",
      "authors": [
        "Lars Schütze",
        "Jeronimo Castrillon"
      ],
      "monthYear": "November 2020",
      "pages": "pp 52–62",
      "shortAbstract": "<p>Adaptive software becomes more and more important as computing is increasingly context-dependent. Runtime adaptability can be achieved by dynamically selecting and applying context-specific code. Role-oriented programming has been proposed as a paradigm ...</p>",
      "citations": 0,
      "downloads": 46,
      "id": 17,
      "proceedingsId": 1,
      "fullAbstract": "\n\t\t<p>Adaptive software becomes more and more important as computing is increasingly context-dependent. Runtime adaptability can be achieved by dynamically selecting and applying context-specific code. Role-oriented programming has been proposed as a paradigm to enable runtime adaptive software by design. Roles change the objects’ behavior at runtime, thus adapting the software to a given context. The cost of adaptivity is however a high runtime overhead stemming from executing compositions of behavior-modifying code. It has been shown that the overhead can be reduced by optimizing dispatch plans at runtime when contexts do not change, but no method exists to reduce the overhead in cases with high context variability. This paper presents a novel approach to implement polymorphic role dispatch, taking advantage of run-time information to effectively guard abstractions and enable reuse even in the presence of variable contexts. The concept of polymorphic inline caches is extended to role invocations. We evaluate the implementation with a benchmark for role-oriented programming languages achieving a geometric mean speedup of 4.0× (3.8× up to 4.5×) with static contexts, and close to no overhead in the case of varying contexts over the current implementation of contextual roles in Object Teams.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "SymJEx: symbolic execution on the GraalVM",
      "url": "https://dl.acm.org/doi/10.1145/3426182.3426187",
      "doi": "10.1145/3426182.3426187",
      "authors": [
        "Sebastian Kloibhofer",
        "Thomas Pointhuber",
        "Maximilian Heisinger",
        "Hanspeter Mössenböck",
        "Lukas Stadler",
        "David Leopoldseder"
      ],
      "monthYear": "November 2020",
      "pages": "pp 63–72",
      "shortAbstract": "<p>Developing software systems is inherently subject to errors that can later cause failures in production. While testing can help to identify critical issues, it is limited to concrete inputs and states. Exhaustive testing is infeasible in practice; hence ...</p>",
      "citations": 1,
      "downloads": 158,
      "id": 18,
      "proceedingsId": 1,
      "fullAbstract": "\n\t\t<p>Developing software systems is inherently subject to errors that can later cause failures in production. While testing can help to identify critical issues, it is limited to concrete inputs and states. Exhaustive testing is infeasible in practice; hence we can never prove the absence of faults. Symbolic execution, i.e., the process of symbolically reasoning about the program state during execution, can inspect the behavior of a system under all possible concrete inputs at run time. It automatically generates logical constraints that match the program semantics and uses theorem provers to verify the existence of error states within the application. This paper presents a novel symbolic execution engine called SymJEx, implemented on top of the multi-language Java Virtual Machine GraalVM. SymJEx uses the Graal compiler's intermediate representation to derive and evaluate path conditions, allowing GraalVM users to leverage the engine to improve software quality. In this work, we show how SymJEx finds non-trivial faults in existing software systems and compare our approach with established symbolic execution engines.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Transparent acceleration of Java-based deep learning engines",
      "url": "https://dl.acm.org/doi/10.1145/3426182.3426188",
      "doi": "10.1145/3426182.3426188",
      "authors": [
        "Athanasios Stratikopoulos",
        "Mihai-Cristian Olteanu",
        "Ian Vaughan",
        "Zoran Sevarac",
        "Nikos Foutris",
        "Juan Fumero",
        "Christos Kotselidis"
      ],
      "monthYear": "November 2020",
      "pages": "pp 73–79",
      "shortAbstract": "<p>The advent of modern cloud services, along with the huge volume of data produced on a daily basis, have increased the demand for fast and efficient data processing. This demand is common among numerous application domains, such as deep learning, data ...</p><p></p>",
      "citations": 0,
      "downloads": 52,
      "id": 19,
      "proceedingsId": 1,
      "fullAbstract": "\n\t\t<p>The advent of modern cloud services, along with the huge volume of data produced on a daily basis, have increased the demand for fast and efficient data processing. This demand is common among numerous application domains, such as deep learning, data mining, and computer vision. In recent years, hardware accelerators have been employed as a means to meet this demand, due to the high parallelism that these applications exhibit. Although this approach can yield high performance, the development of new deep learning neural networks on heterogeneous hardware requires a steep learning curve. The main reason is that existing deep learning engines support the static compilation of the accelerated code, that can be accessed via wrapper calls from a wide range of managed programming languages (e.g., Java, Python, Scala). Therefore, the development of high-performance neural network architectures is fragmented between programming models, thereby forcing developers to manually specialize the code for heterogeneous execution. The specialization of the applications' code for heterogeneous execution is not a trivial task, as it requires developers to have hardware expertise and use a low-level programming language, such as OpenCL, CUDA or High Level Synthesis (HLS) tools. </p> <p> In this paper we showcase how we have employed TornadoVM, a state-of-the-art heterogeneous programming framework to transparently accelerate Deep Netts on heterogeneous hardware. Our work shows how a pure Java-based deep learning neural network engine can be dynamically compiled at runtime and specialized for particular hardware accelerators, without requiring developers to employ any low-level programming framework typically used for such devices. Our preliminary results show up to 6.45x end-to-end performance speedup and up to 88.5x kernel performance speedup, when executing the feed forward process of the network's training on the GPUs against the sequential execution of the original Deep Netts framework.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "You can’t hide you can’t run: a performance assessment of managed applications on a NUMA machine",
      "url": "https://dl.acm.org/doi/10.1145/3426182.3426189",
      "doi": "10.1145/3426182.3426189",
      "authors": [
        "Orion Papadakis",
        "Foivos S. Zakkak",
        "Nikos Foutris",
        "Christos Kotselidis"
      ],
      "monthYear": "November 2020",
      "pages": "pp 80–88",
      "shortAbstract": "<p>The ever-growing demand for more memory capacity from applications has always been a challenging factor in computer architecture. The advent of the Non Unified Memory Access (NUMA) architecture has achieved to work around the physical constraints of a ...</p>",
      "citations": 1,
      "downloads": 70,
      "id": 20,
      "proceedingsId": 1,
      "fullAbstract": "\n\t\t<p>The ever-growing demand for more memory capacity from applications has always been a challenging factor in computer architecture. The advent of the Non Unified Memory Access (NUMA) architecture has achieved to work around the physical constraints of a single processor by providing more system memory using pools of processors, each with their own memory elements, but with variable access times. However, the efficient exploitation of such computing systems is a non-trivial task for software engineers. We have observed that the performance of more than half of the applications picked from two distinct benchmark suites is negatively affected when running on a NUMA machine, in the absence of manual tuning. This finding motivated us to develop a new profiling tool, so called PerfUtil, to study, characterize and better understand why those benchmarks have sub-optimal performance on NUMA machines. PerfUtil's effectiveness is based on its ability to track numerous events throughout the system at the managed runtime system level, that, ultimately, assists in demystifying NUMA peculiarities and accurately characterize managed applications profiles.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "trcview: interactive architecture agnostic execution trace analysis",
      "url": "https://dl.acm.org/doi/10.1145/3426182.3426190",
      "doi": "10.1145/3426182.3426190",
      "authors": [
        "Daniel Pekarek",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "November 2020",
      "pages": "pp 89–97",
      "shortAbstract": "<p>Debuggers are traditionally used to investigate and observe the dynamic behavior of software. Reverse debuggers record a program execution and provide a method to step through the program forward and backward, to quickly locate the operation of ...</p><p></p>",
      "citations": 1,
      "downloads": 36,
      "id": 21,
      "proceedingsId": 1,
      "fullAbstract": "\n\t\t<p>Debuggers are traditionally used to investigate and observe the dynamic behavior of software. Reverse debuggers record a program execution and provide a method to step through the program forward and backward, to quickly locate the operation of interest. However, recording based approaches usually assume a specific method of recording the program execution. Furthermore, the recording and analysis is often linked in a certain way, so that it is not trivial, to quickly add support for new architectures or other recording tools. </p> <p> To solve these shortcomings, we defined a set of essential event types, to fully capture a program execution in a platform-independent way. A prototype of the interactive trace analysis software was implemented in Java, which can handle recorded execution traces with 65 million instructions, when using a Java heap size of 16GiB for the analysis tool. To validate the platform-independence, 3 fundamentally different architectures were tested: AMD64, PowerPC, and PDP-11.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Supporting on-stack replacement in unstructured languages by loop reconstruction and extraction",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361030",
      "doi": "10.1145/3357390.3361030",
      "authors": [
        "Raphael Mosaner",
        "David Leopoldseder",
        "Manuel Rigger",
        "Roland Schatz",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "October 2019",
      "pages": "pp 1–13",
      "shortAbstract": "<p>On-stack replacement (OSR) is a common technique employed by dynamic compilers to reduce program warm-up time. OSR allows switching from interpreted to compiled code during the execution of this code. The main targets are long running loops, which need ...</p>",
      "citations": 2,
      "downloads": 122,
      "id": 22,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>On-stack replacement (OSR) is a common technique employed by dynamic compilers to reduce program warm-up time. OSR allows switching from interpreted to compiled code during the execution of this code. The main targets are long running loops, which need to be represented explicitly, with dedicated information about condition and body, to be optimized at run time. Bytecode interpreters, however, represent control flow implicitly via unstructured jumps and thus do not exhibit the required high-level loop representation. To enable OSR also for jump-based - often called unstructured - languages, we propose the partial reconstruction of loops in order to explicitly represent them in a bytecode interpreter. Besides an outline of the general idea, we implemented our approach in Sulong, a bytecode interpreter for LLVM bitcode, which allows the execution of C/C++. We conducted an evaluation with a set of C benchmarks, which showed speed-ups in warm-up of up to 9x for certain benchmarks. This facilitates execution of programs with long-running loops in rarely called functions, which would yield significant slowdown without OSR. While shown with a prototype implementation, the overall idea of our approach is generalizable for all bytecode interpreters.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "GraalSqueak: toward a smalltalk-based tooling platform for polyglot programming",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361024",
      "doi": "10.1145/3357390.3361024",
      "authors": [
        "Fabio Niephaus",
        "Tim Felgentreff",
        "Robert Hirschfeld"
      ],
      "monthYear": "October 2019",
      "pages": "pp 14–26",
      "shortAbstract": "<p>Polyglot programming provides software developers with a broader choice in terms of software libraries and frameworks available for building applications. Previous research and engineering activities have focused on language interoperability and the ...</p>",
      "citations": 6,
      "downloads": 178,
      "id": 23,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>Polyglot programming provides software developers with a broader choice in terms of software libraries and frameworks available for building applications. Previous research and engineering activities have focused on language interoperability and the design and implementation of fast polyglot runtimes. To make polyglot programming more approachable for developers, novel software development tools are needed that help them build polyglot applications. We believe a suitable prototyping platform helps to more quickly evaluate new ideas for such tools. In this paper we present GraalSqueak, a Squeak/Smalltalk virtual machine implementation for the GraalVM. We report our experience implementing GraalSqueak, evaluate the performance of the language and the programming environment, and discuss how the system can be used as a tooling platform for polyglot programming.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "WARDuino: a dynamic WebAssembly virtual machine for programming microcontrollers",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361029",
      "doi": "10.1145/3357390.3361029",
      "authors": [
        "Robbert Gurdeep Singh",
        "Christophe Scholliers"
      ],
      "monthYear": "October 2019",
      "pages": "pp 27–36",
      "shortAbstract": "<p>It is extremely hard and time-consuming to make correct and efficient programs for microcontrollers. Usually microcontrollers are programmed in a low level programming language such as C which makes them hard to debug and maintain. To raise the ...</p><p></p><p></p>",
      "citations": 6,
      "downloads": 421,
      "id": 24,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>It is extremely hard and time-consuming to make correct and efficient programs for microcontrollers. Usually microcontrollers are programmed in a low level programming language such as C which makes them hard to debug and maintain. To raise the abstraction level, many high level programming languages have provided support for programming microcontrollers. Examples include Python, Lua, C# and JavaScript. Using these languages has the downside that they are orders of magnitude slower than the low-level languages. Moreover, they often provide no remote debugging support. </p> <p>In this paper we investigate the feasibility of using WebAssembly to program Arduino compatible microcontrollers. Our experiments lead to extending the standard WebAssembly VM with: 1) safe live code updates for functions and data 2) remote debugging support at the VM level 3) programmer configurable (Arduino) modules in order to keep the virtual machine’s footprint as small as possible. The resulting WARDuino VM enables the programmer to have better performance than an interpreted approach while simultaneously increasing the ease of development. </p> <p>To evaluate our approach, we implemented a simple breakout game and conducted micro benchmarks which show that the VM runs approximately 5 times faster than Espruino, a popular JavaScript interpreter for the ESP32 microcontroller.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Dynamic one-to-one mapping of ownership records for STM using versioned weak references",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361020",
      "doi": "10.1145/3357390.3361020",
      "authors": [
        "Martin Bättig",
        "Thomas R. Gross"
      ],
      "monthYear": "October 2019",
      "pages": "pp 37–49",
      "shortAbstract": "<p>Software transactional memory (STM) stores information regarding ownership of memory locations in ownership records. We present a scheme to realize a one-to-one mapping of ownership records to memory locations that has moderate memory overhead and is ...</p><p></p><p></p>",
      "citations": 0,
      "downloads": 52,
      "id": 25,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>Software transactional memory (STM) stores information regarding ownership of memory locations in ownership records. We present a scheme to realize a one-to-one mapping of ownership records to memory locations that has moderate memory overhead and is suitable for STM implementations on top of managed runtimes. </p> <p> To reduce memory overhead, STM implementations typically map multiple memory locations to a single ownership record. These one-to-many mappings reduce conflict detection granularity and result in false sharing. Further, one-to-many mappings based on hashes of memory addresses suffer from performance anomalies. The proposed mapping scheme works without knowledge of memory addresses and thus avoids these performance anomalies. The scheme uses weak references and chunking to map memory locations to ownership records. Weak references allow recycling of unused ownership records and thus reduce memory overhead. To enable a fast garbage collection, the scheme uses a versioned managed heap and versioned weak references. Further, we describe optimizations enabled by the one-to-one mapping that compensate overhead introduced by the additional managed heap. </p> <p> An evaluation of the method using the Deuce framework and JStamp benchmarks shows that, on average, the dynamic one-to-one mapping provides better performance than hash-based one-to-many mappings at the cost of a moderate memory overhead.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "A type system for data independence of loop iterations in a directive-based PGAS language",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361021",
      "doi": "10.1145/3357390.3361021",
      "authors": [
        "Tatsuya Abe"
      ],
      "monthYear": "October 2019",
      "pages": "pp 50–62",
      "shortAbstract": "<p>Data independence of iterations of a loop statement in a partitioned global address space (PGAS) language is a sufficient condition to enable parallel processing of the loop iterations on distributed memories. However, checking data independence is ...</p>",
      "citations": 1,
      "downloads": 53,
      "id": 26,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>Data independence of iterations of a loop statement in a partitioned global address space (PGAS) language is a sufficient condition to enable parallel processing of the loop iterations on distributed memories. However, checking data independence is generally difficult. In this paper, we propose the non-interference property of statements and design a sub-language of a directive-based PGAS language XcalableMP with a type system using the notion of vertex centricity. Although data independence and non-interference are generally mutually orthogonal, non-interference of a statement in the sub-language, which can be checked easily on the type system, implies data independence. We also implemented type checking on the Omni compiler for XcalableMP and confirmed the effectiveness of our approach using case studies of directive-based parallelization and temporal blocking optimization of stencil kernels.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Hosting OpenMP programs on Java virtual machines",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361031",
      "doi": "10.1145/3357390.3361031",
      "authors": [
        "Swapnil Gaikwad",
        "Andy Nisbet",
        "Mikel Luján"
      ],
      "monthYear": "October 2019",
      "pages": "pp 63–71",
      "shortAbstract": "<p>To leverage existing virtual machine infrastructures is attractive for programming language implementors because competitive runtime performance may be achieved with a reduced effort. For example, the Truffle framework has enabled Ruby (TruffleRuby), ...</p>",
      "citations": 0,
      "downloads": 97,
      "id": 27,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>To leverage existing virtual machine infrastructures is attractive for programming language implementors because competitive runtime performance may be achieved with a reduced effort. For example, the Truffle framework has enabled Ruby (TruffleRuby), and C (Sulong)guest language implementations to be hosted on a Java Virtual Machine(JVM). In this paper, we present Sulong-OpenMP, the first Truffle-based implementation to support parallel programs written in C/C++ and OpenMP. Our implementation adds OpenMP support to Sulongthat executes LLVM Intermediate Representation (LLVM IR) for C/C++ programs on a JVM. We outline the challenges faced in supporting OpenMP execution semantics, and the current limitations of Sulong-OpenMP. The geometric mean overhead of 1 thread Sulong-OpenMP compared to sequential Sulong execution was 2.6% for the NAS Parallel Benchmark suite, at peak runtime performance. Although this paper focuses on the correctness of our implementation concerning the OpenMP memory model, we also highlight the diminishing performance gap between the native execution with clang -O2 and our Sulong-OpenMP as only 1.2x in the best case using 4 OpenMP threads.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Predicting all data race pairs for a specific schedule",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361022",
      "doi": "10.1145/3357390.3361022",
      "authors": [
        "Martin Sulzmann",
        "Kai Stadtmüller"
      ],
      "monthYear": "October 2019",
      "pages": "pp 72–84",
      "shortAbstract": "<p>We consider the problem of data race prediction where the program's behavior is represented by a trace. A trace is a sequence of program events recorded during the execution of the program. We employ the schedulable happens-before relation to ...</p>",
      "citations": 1,
      "downloads": 62,
      "id": 28,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>We consider the problem of data race prediction where the program's behavior is represented by a trace. A trace is a sequence of program events recorded during the execution of the program. We employ the schedulable happens-before relation to characterize all pairs of events that are in a race for the schedule as manifested in the trace. Compared to the classic happens-before relation, the schedulable happens-before relations properly takes care of write-read dependencies and thus avoids false positives. The challenge is to efficiently identify all (schedulable) data race pairs. We present a refined linear time vector clock algorithm to predict many of the schedulable data race pairs. We introduce a quadratic time post-processing algorithm to predict all remaining data race pairs. This improves the state of the art in the area and our experiments show that our approach scales to real-world examples. Thus, the user can systematically examine and fix all program locations that are in a race for a particular schedule.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Towards efficient, multi-language dynamic taint analysis",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361028",
      "doi": "10.1145/3357390.3361028",
      "authors": [
        "Jacob Kreindl",
        "Daniele Bonetta",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "October 2019",
      "pages": "pp 85–94",
      "shortAbstract": "<p>Dynamic taint analysis is a program analysis technique in which data is marked and its propagation is tracked while the program is executing. It is applied to solve problems in many fields, especially in software security. Current taint analysis ...</p><p></p>",
      "citations": 3,
      "downloads": 275,
      "id": 29,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>Dynamic taint analysis is a program analysis technique in which data is marked and its propagation is tracked while the program is executing. It is applied to solve problems in many fields, especially in software security. Current taint analysis platforms are limited to a single programming language, and therefore cannot support programs which, as is common today, are implemented in multiple programming languages. Current implementations of dynamic taint analysis also incur a significant performance overhead. </p> <p> In this paper we address both these limitations (1) by presenting our vision of a multi-language dynamic taint analysis platform, which is built around a language-agnostic core framework that is extended by language-specific front-ends and (2) by discussing the use of speculative optimization and dynamic compilation to reduce the execution overhead of dynamic taint analysis applications. An implementation of such a platform would enable dynamic taint analyses that can target multiple languages in one analysis implementation and can track tainted data across language boundaries. We describe this approach in the context of the GraalVM runtime and its included JIT compiler, Graal, which allows us to target both dynamic and static languages.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Detection of suspicious time windows in memory monitoring",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361025",
      "doi": "10.1145/3357390.3361025",
      "authors": [
        "Markus Weninger",
        "Elias Gander",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "October 2019",
      "pages": "pp 95–104",
      "shortAbstract": "<p>Modern memory monitoring tools do not only offer analyses at a single point in time, but also offer features to analyze the memory evolution over time. These features provide more detailed insights into an application's behavior, yet they also make the ...</p><p></p><p></p><p></p>",
      "citations": 4,
      "downloads": 78,
      "id": 30,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>Modern memory monitoring tools do not only offer analyses at a single point in time, but also offer features to analyze the memory evolution over time. These features provide more detailed insights into an application's behavior, yet they also make the tools more complex and harder to use. </p> <p> Analyses over time are typically performed on certain time windows within which the application behaves abnormally. Such suspicious time windows first have to be detected by the users, which is a non-trivial task, especially for novice users that have no experience in memory monitoring. </p> <p> In this paper, we present algorithms to automatically detect suspicious time windows that exhibit (1) continuous memory growth, (2) high GC utilization, or (3) high memory churn. For each of these problems we also discuss its root causes and implications. </p> <p> To show the feasibility of our detection techniques, we integrated them into AntTracks, a memory monitoring tool developed by us. Throughout the paper, we present their usage on various problems and real-world applications.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Static TypeScript: an implementation of a static compiler for the TypeScript language",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361032",
      "doi": "10.1145/3357390.3361032",
      "authors": [
        "Thomas Ball",
        "Peli de Halleux",
        "Michał Moskal"
      ],
      "monthYear": "October 2019",
      "pages": "pp 105–116",
      "shortAbstract": "<p>While the programming of microcontroller-based embeddable devices typically is the realm of the C language, such devices are now finding their way into the classroom for CS education, even at the level of middle school. As a result, the use of scripting ...</p><p></p><p></p>",
      "citations": 4,
      "downloads": 189,
      "id": 31,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>While the programming of microcontroller-based embeddable devices typically is the realm of the C language, such devices are now finding their way into the classroom for CS education, even at the level of middle school. As a result, the use of scripting languages (such as JavaScript and Python) for microcontrollers is on the rise. </p> <p> We present Static TypeScript (STS), a subset of TypeScript (itself, a gradually typed superset of JavaScript), and its compiler/linker toolchain, which is implemented fully in TypeScript and runs in the web browser. STS is designed to be useful in practice (especially in education), while being amenable to static compilation targeting small devices. A user’s STS program is compiled to machine code in the browser and linked against a precompiled C++ runtime, producing an executable that is more efficient than the prevalent embedded interpreter approach, extending battery life and making it possible to run on devices with as little as 16 kB of RAM (such as the BBC micro:bit). </p> <p> This paper is primarily a description of the STS system and the technical challenges of implementing embedded programming platforms in the classroom.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "PorcE: a deparallelizing compiler",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361023",
      "doi": "10.1145/3357390.3361023",
      "authors": [
        "Arthur Michener Peters",
        "John A. Thywissen",
        "Christopher J. Rossbach"
      ],
      "monthYear": "October 2019",
      "pages": "pp 117–130",
      "shortAbstract": "<p>Concurrent and parallel programming environments must balance three competing goals: performance, productivity, and generality. Most current environments take a sequential specification and parallelize it through a series of program analyses and ...<em></em><em></em><em></em></p><p><em></em></p>",
      "citations": 0,
      "downloads": 63,
      "id": 32,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>Concurrent and parallel programming environments must balance three competing goals: performance, productivity, and generality. Most current environments take a sequential specification and parallelize it through a series of program analyses and transformations. Programmer <i>productivity</i> is determined by how much the programmer is involved in specifying the transform. A programming environment’s <i>generality</i> is determined by how much it restricts its programming model to enable the transform to be more automatic or provide better performance. Productivity and generality impact the <i>performance</i> of the resulting code, giving rise to a performance–productivity–generality trade-off space. </p> <p>PorcE takes a different approach: PorcE starts from a maximally concurrent program, which it <i>de</i>parallelizes. We hypothesize that this dramatically changes the accessible regions of the performance–productivity–generality space, because the complexity of deparallelization is quite different than for parallelization. PorcE uses a novel combination of optimizations that enable multicore scaling with performance similar to popular high-level languages, such as Python. Benchmarks show that optimized PorcE is 56&nbsp;times faster than unoptimized excessively parallel PorcE. It is 4&nbsp;times slower than hand-parallelized Scala, on average, when using existing Scala code for core sequential computations, which is similar to Python. This shows the potential for practical performance of pervasively concurrent programs.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "An analysis of call-site patching without strong hardware support for self-modifying-code",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361027",
      "doi": "10.1145/3357390.3361027",
      "authors": [
        "Tim Hartley",
        "Foivos S. Zakkak",
        "Christos Kotselidis",
        "Mikel Luján"
      ],
      "monthYear": "October 2019",
      "pages": "pp 131–143",
      "shortAbstract": "<p>With micro-services continuously gaining popularity and low-power processors making their way into data centers, efficient execution of managed runtime systems on low-power architectures is also gaining interest. Apart from the inherent performance ...</p>",
      "citations": 0,
      "downloads": 97,
      "id": 33,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>With micro-services continuously gaining popularity and low-power processors making their way into data centers, efficient execution of managed runtime systems on low-power architectures is also gaining interest. Apart from the inherent performance differences between high and low power processors, porting a managed runtime system to a low-power architecture may result in spuriously introducing additional overheads and design trade-offs. In this work we investigate how the lack of strong hardware support for Self Modifying Code (SMC) in low-power architectures, influences Just-In-Time (JIT) compilation and execution in modern virtual machines. In particular, we examine how low-power architectures, with no or limited hardware support for SMC, impose restrictions on call-site implementations, when the latter need to be patchable by the runtime system. We present four different memory-safe implementations for call-site generation and discuss their advantages and disadvantages in the absence of strong hardware support for SMC. Finally, we evaluate each technique on different workloads using micro-benchmarks and we evaluate the best two techniques on the Dacapo benchmark suite showcasing performance differences up to 15%.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Performance of an OO compute kernel on the JVM: revisiting Java as a language for scientific computing applications",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361026",
      "doi": "10.1145/3357390.3361026",
      "authors": [
        "Malin Källén",
        "Tobias Wrigstad"
      ],
      "monthYear": "October 2019",
      "pages": "pp 144–156",
      "shortAbstract": "<p>The study of Java as a programming language for scientific computing is warranted by simpler, more extensible and more easily maintainable code. Previous work on refactoring a C++ scientific computing code base to follow best practises of object-...</p>",
      "citations": 1,
      "downloads": 101,
      "id": 34,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>The study of Java as a programming language for scientific computing is warranted by simpler, more extensible and more easily maintainable code. Previous work on refactoring a C++ scientific computing code base to follow best practises of object-oriented software development revealed a coupling of such practises and considerable slowdowns due to indirections introduced by abstractions. In this paper, we explore how Java's JIT compiler handle such abstraction-induced indirection using a typical scientific computing compute kernel extracted from a linear solver written in C++. We find that the computation times for large workloads on one machine can be on-pair for C++ and Java. However, for distributed computations, a better parallelisation strategy needs to be found for non-blocking communication. We also report on the impact on performance for common \"gripes\": garbage collection, array bounds checking, and dynamic binding.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Asynchronous snapshots of actor systems for latency-sensitive applications",
      "url": "https://dl.acm.org/doi/10.1145/3357390.3361019",
      "doi": "10.1145/3357390.3361019",
      "authors": [
        "Dominik Aumayr",
        "Stefan Marr",
        "Elisa Gonzalez Boix",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "October 2019",
      "pages": "pp 157–171",
      "shortAbstract": "<p>The actor model is popular for many types of server applications. Efficient snapshotting of applications is crucial in the deployment of pre-initialized applications or moving running applications to different machines, e.g for debugging purposes. A key ...</p>",
      "citations": 1,
      "downloads": 88,
      "id": 35,
      "proceedingsId": 2,
      "fullAbstract": "\n\t\t<p>The actor model is popular for many types of server applications. Efficient snapshotting of applications is crucial in the deployment of pre-initialized applications or moving running applications to different machines, e.g for debugging purposes. A key issue is that snapshotting blocks all other operations. In modern latency-sensitive applications, stopping the application to persist its state needs to be avoided, because users may not tolerate the increased request latency. In order to minimize the impact of snapshotting on request latency, our approach persists the application’s state asynchronously by capturing partial heaps, completing snapshots step by step. Additionally, our solution is transparent and supports arbitrary object graphs. We prototyped our snapshotting approach on top of the Truffle/Graal platform and evaluated it with the Savina benchmarks and the Acme Air microservice application. When performing a snapshot every thousand Acme Air requests, the number of slow requests ( 0.007% of all requests) with latency above 100ms increases by 5.43%. Our Savina microbenchmark results detail how different utilization patterns impact snapshotting cost. To the best of our knowledge, this is the first system that enables asynchronous snapshotting of actor applications, i.e. without stop-the-world synchronization, and thereby minimizes the impact on latency. We thus believe it enables new deployment and debugging options for actor systems.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Run-time program-specific phase prediction for python programs",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237011",
      "doi": "10.1145/3237009.3237011",
      "authors": [
        "Meng-Chieh Chiu",
        "Eliot Moss"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 1",
      "shortAbstract": "<p>It is well-known that a program's execution can be partitioned into different phases. Because of their impact on micro-architectural components such as caches and branch predictors, phases are an important aspect of dynamic program behavior that is ...</p><p><em></em><em></em></p>",
      "citations": 1,
      "downloads": 110,
      "id": 36,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>It is well-known that a program's execution can be partitioned into different phases. Because of their impact on micro-architectural components such as caches and branch predictors, phases are an important aspect of dynamic program behavior that is useful in analyzing performance [Xian et al., 2007, Roh et al., 2009, Gu and Verbrugge, 2008] and energy consumption. They are also relevant to detecting whether a program is executing as expected, which helps software engineers in program analysis and understanding [Peleg and Mendelson, 2007, Singer and Kirkham, 2008, Pirzadeh et al., 2011, Benomar et al., 2014].</p> <p>We develop here a method for run-time phase prediction for Python programs. After training on different runs of each program of interest, our method can predict the future phase <i>k</i> steps later, for <i>k</i> = 1, ..., at run time, for a new run of the same program but on different inputs, with good precision and recall (compared with a \"ground truth\" definition of phases) and with small run-time overhead (average less than 1%).</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Fast-path loop unrolling of non-counted loops to enable subsequent compiler optimizations",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237013",
      "doi": "10.1145/3237009.3237013",
      "authors": [
        "David Leopoldseder",
        "Roland Schatz",
        "Lukas Stadler",
        "Manuel Rigger",
        "Thomas Würthinger",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 2",
      "shortAbstract": "<p>Java programs can contain non-counted loops, that is, loops for which the iteration count can neither be determined at compile time nor at run time. State-of-the-art compilers do not aggressively optimize them, since unrolling non-counted loops often ...</p><p></p><p></p>",
      "citations": 3,
      "downloads": 159,
      "id": 37,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Java programs can contain non-counted loops, that is, loops for which the iteration count can neither be determined at compile time nor at run time. State-of-the-art compilers do not aggressively optimize them, since unrolling non-counted loops often involves duplicating also a loop's exit condition, which thus only improves run-time performance if subsequent compiler optimizations can optimize the unrolled code.</p> <p>This paper presents an unrolling approach for non-counted loops that uses simulation at run time to determine whether unrolling such loops enables subsequent compiler optimizations. Simulating loop unrolling allows the compiler to determine performance and code size effects for each potential transformation prior to performing it.</p> <p>We implemented our approach on top of the GraalVM, a high-performance virtual machine for Java, and evaluated it with a set of Java and JavaScript benchmarks in terms of peak performance, compilation time and code size increase. We show that our approach can improve performance by up to 150% while generating a median code size and compile-time increase of not more than 25%. Our results indicate that fast-path unrolling of non-counted loops can be used in practice to increase the performance of Java applications.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Redundancy elimination in the presence of split class initialization",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237014",
      "doi": "10.1145/3237009.3237014",
      "authors": [
        "Artur Pilipenko",
        "Oleg Pliss"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 3",
      "shortAbstract": "<p>Virtual machines for mobile and embedded devices often use the romization technique when the VM is loaded from a preinitialized state, previously saved as an image. This way some of the initialization work can be avoided in a resource constrained ...</p><p></p><p></p><p></p><p></p>",
      "citations": 1,
      "downloads": 44,
      "id": 38,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Virtual machines for mobile and embedded devices often use the romization technique when the VM is loaded from a preinitialized state, previously saved as an image. This way some of the initialization work can be avoided in a resource constrained environment. For Java, one of the initialization activities which can be moved from execution time to romization time is class initialization.</p> <p>The preinitialized image can be optimized in different ways. For instance, redundancy elimination transformations, which include elimination of unused methods, fields, and classes, can be applied to reduce the static and the dynamic footprint of the image. We consider the problem of redundancy elimination in the presence of romization time class initialization.</p> <p>There is a circular dependency between class initialization and method reachability analysis. On the one hand, class initialization reduces the set of reachable methods as the methods used solely for the initialization become unreachable. On the other hand, eager initialization of classes which are not used by the reachable methods can increase the size of the image by creating reachable but unused objects in the heap.</p> <p>We propose a method reachability analysis algorithm which breaks this circular dependency by performing selective class initialization during the analysis. The algorithm keeps track of the classes which can be initialized by the reachable methods and initializes a subset of these classes. A simple heuristic is used to choose the classes which are safe to preinitialize.</p> <p>We also found that elimination of initialized but unused reference fields in Java can affect finalization semantics. The elimination of a field might cause some objects to be collected by the GC, which can be observed by the application. We present an algorithm for elimination of unused fields which preserves finalization behavior.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Exploiting high-performance heterogeneous hardware for Java programs using graal",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237016",
      "doi": "10.1145/3237009.3237016",
      "authors": [
        "James Clarkson",
        "Juan Fumero",
        "Michail Papadimitriou",
        "Foivos S. Zakkak",
        "Maria Xekalaki",
        "Christos Kotselidis",
        "Mikel Luján"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 4",
      "shortAbstract": "<p>The proliferation of heterogeneous hardware in recent years means that every system we program is likely to include a mix of compute elements; each with different characteristics. By utilizing these available hardware resources, developers can improve ...</p><p><em></em></p><p></p>",
      "citations": 10,
      "downloads": 163,
      "id": 39,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>The proliferation of heterogeneous hardware in recent years means that every system we program is likely to include a mix of compute elements; each with different characteristics. By utilizing these available hardware resources, developers can improve the performance and energy efficiency of their applications. However, existing tools for heterogeneous programming neglect developers who do not have the time or inclination to switch programming languages or learn the intricacies of a specific piece of hardware.</p> <p>This paper presents a framework that enables Java applications to be deployed across a variety of heterogeneous systems while exploiting any available multi- or many-core processor. The novel aspect of our approach is that it does not require any <i>a priori</i> knowledge of the hardware, or for the developer to worry about managing disparate memory spaces. Java applications are transparently compiled and optimized for the hardware at run-time.</p> <p>We also present a performance evaluation of our just-in-time (JIT) compiler using a framework to accelerate SLAM, a complex computer vision application entirely written in Java. We show that we can accelerate SLAM up to 150x compared to the Java reference implementation, rendering 107 frames per second (FPS).</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Performance analysis for languages hosted on the truffle framework",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237019",
      "doi": "10.1145/3237009.3237019",
      "authors": [
        "Swapnil Gaikwad",
        "Andy Nisbet",
        "Mikel Luján"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 5",
      "shortAbstract": "<p>It is attractive to host new or existing language implementations on top of, or reusing components of, existing managed language runtimes such as the Java Virtual Machine (JVM) or the Microsoft Common Language Infrastructure (CLI). A benefit is that ...</p><p><em></em><em></em></p><p><em></em><em></em><em></em><em></em></p>",
      "citations": 3,
      "downloads": 171,
      "id": 40,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>It is attractive to host new or existing language implementations on top of, or reusing components of, existing managed language runtimes such as the Java Virtual Machine (JVM) or the Microsoft Common Language Infrastructure (CLI). A benefit is that software development effort may be reduced, as only one managed language runtime needs to be optimised and maintained, instead of a separate compiler/runtime for each language implementation.</p> <p>For example, the Truffle framework combined with a JVM offers support for executing Javascript, Ruby, R, LLVM IR-compiled languages, etc., as well as supporting the execution of applications combining multiple programming languages within a <i>Polyglot application.</i> In trying to understand the runtime performance of <i>Sulong</i> (i.e. the Truffle project which enables LLVM IR execution), we found a lack of tools and guidance. A similar situation is found for benchmarks written in Ruby and R when executed as Truffle hosted languages. Further, it is non-trivial to relate performance back to the hosted language source code, and to determine whether JVM service overheads, such as garbage collection or JIT compilation, are significant.</p> <p>We describe how to visually analyse the performance of Truffle-hosted languages based on <i>Flamegraphs</i>, allowing time to be related to sampled call-stacks. We use the Linux tool <i>perf</i> and the JVM agent <i>perf-map-agent</i>, along with enhancements to the Graal JIT compiler that map sampled call-stacks onto JVM-hosted <i>guest</i> language source code. This paper demonstrates the ease and flexibility of using these modified tools, with low overhead during execution time. We also illustrate applicability of the techniques to understand the performance of Polyglot applications.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Sparse matrices on the web: Characterizing the performance and optimal format selection of sparse matrix-vector multiplication in JavaScript and WebAssembly",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237020",
      "doi": "10.1145/3237009.3237020",
      "authors": [
        "Prabhjot Sandhu",
        "David Herrera",
        "Laurie Hendren"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 6",
      "shortAbstract": "<p>JavaScript is the most widely used language for web programming, and now increasingly becoming popular for high performance computing, data-intensive applications, and deep learning. More recently, WebAssembly has been introduced as a typed low-level ...</p><p><em></em></p><p></p>",
      "citations": 4,
      "downloads": 159,
      "id": 41,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>JavaScript is the most widely used language for web programming, and now increasingly becoming popular for high performance computing, data-intensive applications, and deep learning. More recently, WebAssembly has been introduced as a typed low-level bytecode representation which promises to enable better performance. Sparse matrix-vector multiplication (SpMV) is an important kernel that is considered critical for the performance of compute-intensive applications. In SpMV, the optimal selection of storage format is one of the key aspects of enabling the best performance.</p> <p>This paper describes the distinctive nature of the performance and choice of optimal sparse matrix storage format for sequential SpMV for the managed languages JavaScript and WebAssembly, as compared to native languages like C. We performed exhaustive experiments with 2000 real-life sparse matrices. To evaluate the experimental data in a rigorous manner we introduced the notion of <i>x%-affinity</i> which allows us to identify with certainty those storage formats that are at least x% better than all other formats.</p> <p>We explored three main research questions. First, we examined the difference in performance between native C and both JavaScript and WebAssembly, for two major browsers, Firefox and Chrome. For JavaScript, we observed that the best performing browser demonstrated a slowdown of only 2.2x to 5.8x versus C. Somewhat surprisingly, for WebAssembly, we observed similar or better performance as compared to C, for the best performing browser. Second, we explored the performance of single-precision versus double-precision SpMV. In contrast to C, in JavaScript and WebAssembly, we found that double-precision is often more efficient than single-precision. Lastly, we examined the choice of optimal storage format. Interestingly, the best format choices are very different for C as compared to both JavaScript and WebAssembly, and even quite different between the two browsers.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Parallel trace register allocation",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237010",
      "doi": "10.1145/3237009.3237010",
      "authors": [
        "Josef Eisl",
        "David Leopoldseder",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 7",
      "shortAbstract": "<p>Register allocation is a mandatory task for almost every compiler and consumes a significant portion of compile time. In a just-in-time compiler, compile time is a particular issue because compilation happens during program execution and contributes to ...</p><p></p>",
      "citations": 0,
      "downloads": 85,
      "id": 42,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Register allocation is a mandatory task for almost every compiler and consumes a significant portion of compile time. In a just-in-time compiler, compile time is a particular issue because compilation happens during program execution and contributes to the overall application run time. Parallelization can help here. We developed a theoretical model for parallel register allocation and show that it can be used in practice without a negative impact on the quality of the allocation result. Doing so reduces compilation latency, i.e., the duration until the result of a compilation is available.</p> <p>Our analysis shows that parallelization can theoretically decrease allocation latency by almost 50%. We implemented an initial prototype which reduces the register allocation latency by 28% when using four threads, compared to the single-threaded allocation.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Alto: lightweight VMs using virtualization-aware managed runtimes",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237022",
      "doi": "10.1145/3237009.3237022",
      "authors": [
        "James Larisch",
        "James Mickens",
        "Eddie Kohler"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 8",
      "shortAbstract": "<p>Virtualization enables datacenter operators to safely run computations that belong to untrusted tenants. An ideal virtual machine has three properties: a small memory footprint; strong isolation from other VMs and the host OS; and the ability to ...<em></em></p>",
      "citations": 6,
      "downloads": 131,
      "id": 43,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Virtualization enables datacenter operators to safely run computations that belong to untrusted tenants. An ideal virtual machine has three properties: a small memory footprint; strong isolation from other VMs and the host OS; and the ability to maintain in-memory state across client requests. Unfortunately, modern virtualization technologies cannot provide all three properties at once. In this paper, we explain why, and propose a new virtualization approach, called Alto, that virtualizes at the layer of a <i>managed runtime interface.</i> Through careful design of (1) the application-facing managed interface and (2) the internal runtime architecture, Alto provides VMs that are small, secure, and stateful. Conveniently, Alto also simplifies VM operations like suspension, migration, and resumption. We provide several details about the proposed design, and discuss the remaining challenges that must be solved to fully realize the Alto vision.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Towards a framework for stochastic performance optimizations in compilers and interpreters: an architecture overview",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237024",
      "doi": "10.1145/3237009.3237024",
      "authors": [
        "Oliver Krauss"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 9",
      "shortAbstract": "<p>Modern compilers and interpreters provide code optimizations before and during run-time to stay competitive with alternative execution environments, thus moving required domain knowledge about the compilation process away from the developer and speeding ...</p><p></p><p></p>",
      "citations": 2,
      "downloads": 78,
      "id": 44,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Modern compilers and interpreters provide code optimizations before and during run-time to stay competitive with alternative execution environments, thus moving required domain knowledge about the compilation process away from the developer and speeding up resulting software. These optimizations are often based on formal proof, or alternatively have recovery paths as backup.</p> <p>This publication proposes an architecture utilizing abstract syntax trees (ASTs) to optimize the runtime performance of code with stochastic - search based - machine learning techniques. From these AST modifying optimizations a pattern mining approach attempts to find transformation patterns which are applicable to a software language. The application of these patterns happens during the parsing process or the programs run-time.</p> <p>Future work consists of implementing and extending the presented architecture, with a considerable focus on the mining of transformation patterns.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Specializing ropes for ruby",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237026",
      "doi": "10.1145/3237009.3237026",
      "authors": [
        "Kevin Menard",
        "Chris Seaton",
        "Benoit Daloze"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 10",
      "shortAbstract": "<p>Ropes are an immutable data structure for representing character strings via a binary tree of operation-labeled nodes. Ropes were designed to perform well with large strings, and in particular, concatenation of large strings. We present our findings in ...</p>",
      "citations": 1,
      "downloads": 74,
      "id": 45,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Ropes are an immutable data structure for representing character strings via a binary tree of operation-labeled nodes. Ropes were designed to perform well with large strings, and in particular, concatenation of large strings. We present our findings in using ropes to implement mutable strings in TruffleRuby, an implementation of the Ruby programming language using a self-specializing abstract syntax tree interpreter and dynamic compilation. We extend ropes to support Ruby language features such as encodings and refine operations to better support typical Ruby programs. Finally, we evaluate the performance of our implementation of ropes and demonstrate that they perform 0.9X − 9.4X as fast as byte array-based strings in benchmarks of common Ruby string operations.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Defining a high-level programming model for emerging NVRAM technologies",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237027",
      "doi": "10.1145/3237009.3237027",
      "authors": [
        "Thomas Shull",
        "Jian Huang",
        "Josep Torrellas"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 11",
      "shortAbstract": "<p>Byte-addressable non-volatile memory is poised to become prevalent in the near future. Thanks to device-level technological advances, hybrid systems of traditional dynamic random-access memory (DRAM) coupled with non-volatile random-access memory (NVRAM)...</p><p></p><p></p><p></p>",
      "citations": 8,
      "downloads": 144,
      "id": 46,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Byte-addressable non-volatile memory is poised to become prevalent in the near future. Thanks to device-level technological advances, hybrid systems of traditional dynamic random-access memory (DRAM) coupled with non-volatile random-access memory (NVRAM) are already present and are expected to be commonplace soon. NVRAM offers orders of magnitude performance improvements over existing storage devices. Due to NVRAM's low overheads, many future applications are expected to leverage the fine-grain durable storage provided by NVRAM.</p> <p>Many frameworks for programming NVRAM have been proposed. Unfortunately, these existing frameworks closely mirror the underlying hardware. This lack of abstraction hurts programmer productivity, makes it easy to write buggy code, and limits the compiler's effectiveness. Furthermore, this low level of abstraction does not match the expectations of managed language users.</p> <p>To rectify this situation, in this paper we describe a new high-level NVRAM programming model amenable to managed languages. Because our model is defined at a high level, it is intuitive, not prone to user bugs, and is flexible enough to allow language implementers to perform many optimizations while still adhering to the model.</p> <p>In addition to proposing this model, we also briefly describe how Java can be extended to support our new model. Finally, we present some initial results on the performance overheads of creating durable applications in NVRAM and describe what future work we intend to complete.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Debugging native extensions of dynamic languages",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237017",
      "doi": "10.1145/3237009.3237017",
      "authors": [
        "Jacob Kreindl",
        "Manuel Rigger",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 12",
      "shortAbstract": "<p>Many dynamic programming languages such as Ruby and Python enable developers to use so called native extensions, code implemented in typically statically compiled languages like C and C++. However, debuggers for these dynamic languages usually lack ...</p>",
      "citations": 1,
      "downloads": 83,
      "id": 47,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Many dynamic programming languages such as Ruby and Python enable developers to use so called native extensions, code implemented in typically statically compiled languages like C and C++. However, debuggers for these dynamic languages usually lack support for also debugging these native extensions. GraalVM can execute programs implemented in various dynamic programming languages and, by using the LLVM-IR interpreter Sulong, also their native extensions. We added support for source-level debugging to Sulong based on GraalVM's debugging framework by associating run-time debug information from the LLVM-IR level to the original program code. As a result, developers can now use GraalVM to debug source code written in multiple LLVM-based programming languages as well as programs implemented in various dynamic languages that invoke it in a common debugger front-end.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "A blast from the past: online time-travel debugging with BITE",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237012",
      "doi": "10.1145/3237009.3237012",
      "authors": [
        "Stefan Schulz",
        "Christoph Bockisch"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 13",
      "shortAbstract": "<p>Conventional back-in-time debuggers fashion a record and replay approach to facilitate inspecting all events in the past execution. This usually implies a significant performance overhead for tracing and storing information. Furthermore the recording ...</p><p><em></em></p><p><em></em><em></em></p>",
      "citations": 0,
      "downloads": 63,
      "id": 48,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Conventional back-in-time debuggers fashion a record and replay approach to facilitate inspecting all events in the past execution. This usually implies a significant performance overhead for tracing and storing information. Furthermore the recording only becomes available after the monitored code has terminated. In other words, conventional back-in-time-debuggers operate offline.</p> <p>In this paper we present a novel approach of online back-in-time debugging, called <i>Back-in-Time Evaluation (BITE).</i> It offers the capabilities to evaluate code-snippets in the context of previous execution states. Thus, we can focus the trace recording on execution contexts accessible in online debugging, to meet the higher requirements for responsiveness of the debugger.</p> <p>For this purpose, <i>BITE</i> only collects the results of operations that potentially yield different results when they are reevaluated. These are initially stored in the call frame of the executing method, which has a performance overhead of at most 5%. The downside is that once a frame is popped from the stack, the contained trace is disposed of as well. Therefore, <i>BITE</i> also allows to persist the trace collected in a frame before it is popped, at the cost of potentially increasing the overhead. The performance impact of this depends on the amount of trace information that needs to be persisted, as well as the strategy for storing this information. When storing the information in-memory, the overhead induced ranges from having no additional impact at all, up to a factor of 50x. Since memory capabilities are not unlimited, our approach allows for optionally serializing these objects. However, this induces a higher performance overhead.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Utilizing object reference graphs and garbage collection roots to detect memory leaks in offline memory monitoring",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237023",
      "doi": "10.1145/3237009.3237023",
      "authors": [
        "Markus Weninger",
        "Elias Gander",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 14",
      "shortAbstract": "<p>Complex software systems often suffer from performance problems caused by memory anomalies such as memory leaks. While the proliferation of objects is rather easy to detect using state-of-the-art memory monitoring tools, extracting a leak's root cause, ...<em></em><em></em></p>",
      "citations": 6,
      "downloads": 164,
      "id": 49,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Complex software systems often suffer from performance problems caused by memory anomalies such as memory leaks. While the proliferation of objects is rather easy to detect using state-of-the-art memory monitoring tools, extracting a leak's root cause, i.e., identifying the objects that keep the accumulating objects alive, is still poorly supported. Most state-of-the-art tools rely on the dominator tree of the object graph and thus only support single-object ownership analysis. Multi-object ownership analysis, e.g., when the leaking objects are contained in multiple collections, is not possible by merely relying on the dominator tree. We present an efficient approach to continuously collect GC root information (e.g., static fields or thread-local variables) in a trace-based memory monitoring tool, as well as algorithms that use this information to calculate the <i>transitive closure</i> (i.e., all reachable objects) and the <i>GC closure</i> (i.e., objects that are kept alive) for arbitrary heap object groups. These closures allow to derive various metrics for heap object groups that can be used to guide the user during memory leak analysis. We implemented our approach in AntTracks, an offline memory monitoring tool, and demonstrate its usefulness by comparing it with other widely used tools for memory leak detection such as the Eclipse Memory Analyzer. Our evaluation shows that collecting GC root information tracing introduces about 1% overhead, in terms of run time as well as trace file size.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Efficient and deterministic record & replay for actor languages",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237015",
      "doi": "10.1145/3237009.3237015",
      "authors": [
        "Dominik Aumayr",
        "Stefan Marr",
        "Clément Béra",
        "Elisa Gonzalez Boix",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 15",
      "shortAbstract": "<p>With the ubiquity of parallel commodity hardware, developers turn to high-level concurrency models such as the actor model to lower the complexity of concurrent software. However, debugging concurrent software is hard, especially for concurrency models ...</p><p></p>",
      "citations": 9,
      "downloads": 128,
      "id": 50,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>With the ubiquity of parallel commodity hardware, developers turn to high-level concurrency models such as the actor model to lower the complexity of concurrent software. However, debugging concurrent software is hard, especially for concurrency models with a limited set of supporting tools. Such tools often deal only with the underlying threads and locks, which obscures the view on e.g. actors and messages and thereby introduces additional complexity.</p> <p>To improve on this situation, we present a low-overhead record &amp; replay approach for actor languages. It allows one to debug concurrency issues deterministically based on a previously recorded trace. Our evaluation shows that the average run-time overhead for tracing on benchmarks from the Savina suite is 10% (min. 0%, max. 20%). For Acme-Air, a modern web application, we see a maximum increase of 1% in latency for HTTP requests and about 1.4 MB/s of trace data. These results are a first step towards deterministic replay debugging of actor systems in production.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Specializing a meta-interpreter: JIT compilation of dynsem specifications on the graal VM",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237018",
      "doi": "10.1145/3237009.3237018",
      "authors": [
        "Vlad Vergu",
        "Eelco Visser"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 16",
      "shortAbstract": "<p>DynSem is a domain-specific language for concise specification of the dynamic semantics of programming languages, aimed at rapid experimentation and evolution of language designs. DynSem specifications can be executed to interpret programs in the ...</p><p></p>",
      "citations": 2,
      "downloads": 163,
      "id": 51,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>DynSem is a domain-specific language for concise specification of the dynamic semantics of programming languages, aimed at rapid experimentation and evolution of language designs. DynSem specifications can be executed to interpret programs in the language under development. To enable fast turnaround during language development, we have developed a meta-interpreter for DynSem specifications, which requires minimal processing of the specification. In addition to fast development time, we also aim to achieve fast run times for interpreted programs.</p> <p>In this paper we present the design of a meta-interpreter for DynSem and report on experiments with JIT compiling the application of the meta-interpreter on the Graal VM. By interpreting specifications directly, we have minimal compilation overhead. By specializing pattern matches, maintaining call-site dispatch chains and using native control-flow constructs we gain significant run-time performance. We evaluate the performance of the meta-interpreter when applied to the Tiger language specification running a set of common benchmark programs. Specialization enables the Graal VM to JIT compile the meta-interpreter giving speedups of up to factor 15 over running on the standard Oracle Java VM.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "GUARDIA: specification and enforcement of javascript security policies without VM modifications",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237025",
      "doi": "10.1145/3237009.3237025",
      "authors": [
        "Angel Luis Scull Pupo",
        "Jens Nicolay",
        "Elisa Gonzalez Boix"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 17",
      "shortAbstract": "<p>The complex architecture of browser technologies and dynamic characteristics of JavaScript make it difficult to ensure security in client-side web applications. Browser-level security policies alone are not sufficient because it is difficult to apply ...</p><p></p>",
      "citations": 3,
      "downloads": 93,
      "id": 52,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>The complex architecture of browser technologies and dynamic characteristics of JavaScript make it difficult to ensure security in client-side web applications. Browser-level security policies alone are not sufficient because it is difficult to apply them correctly and they can be bypassed. As a result, they need to be completed by application-level security policies.</p> <p>In this paper, we survey existing solutions for specifying and enforcing application-level security policies for client-side web applications, and distill a number of desirable features. Based on these features we developed Guardia, a framework for declaratively specifying and dynamically enforcing application-level security policies for JavaScript web applications without requiring VM modifications. We describe Guardia enforcement mechanism by means of JavaScript reflection with respect to three important security properties (transparency, tamper-proofness, and completeness). We also use Guardia to specify and deploy 12 access control policies discussed in related work in three experimental applications that are representative of real-world applications. Our experiments indicate that Guardia is correct, transparent, and tamper-proof, while only incurring a reasonable runtime overhead.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Software vector chaining",
      "url": "https://dl.acm.org/doi/10.1145/3237009.3237021",
      "doi": "10.1145/3237009.3237021",
      "authors": [
        "M. Anton Ertl"
      ],
      "monthYear": "September 2018",
      "pages": "Article No.: 18",
      "shortAbstract": "<p>Providing vectors of run-time determined length as opaque value types is a good interface between the machine-level SIMD instructions and portable application-oriented programming languages. Implementing vector operations requires a loop that breaks the ...</p>",
      "citations": 0,
      "downloads": 53,
      "id": 53,
      "proceedingsId": 3,
      "fullAbstract": "\n\t\t<p>Providing vectors of run-time determined length as opaque value types is a good interface between the machine-level SIMD instructions and portable application-oriented programming languages. Implementing vector operations requires a loop that breaks the vector into SIMD-register-sized chunks. A compiler can fuse the loops of several vector operations together. However, during normal compilation this is only easy if no other control structures are involved. This paper explores an alternative: collect a trace of vector operations at run-time (following the program control flow during this collecting step), and then perform the combined vector loop. This arrangement has a certain run-time overhead, but its implementation is simpler and can happen independently, in a library. Preliminary performance results indicate that the overhead makes this approach beneficial only for long vectors (&gt; 1KB). For shorter vectors, unfused loops should be used in a library setting. Fortunately, this choice can be made at run time, individually for each vector operation.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Sista: Saving Optimized Code in Snapshots for Fast Start-Up",
      "url": "https://dl.acm.org/doi/10.1145/3132190.3132201",
      "doi": "10.1145/3132190.3132201",
      "authors": [
        "Clément Béra",
        "Eliot Miranda",
        "Tim Felgentreff",
        "Marcus Denker",
        "Stéphane Ducasse"
      ],
      "monthYear": "September 2017",
      "pages": "pp 1–11",
      "shortAbstract": "<p>Modern virtual machines for object-oriented languages such as Java HotSpot, Javascript V8 or Python PyPy reach high performance through just-in-time compilation techniques, involving on-the-fly optimization and deoptimization of the executed code. These ...</p>",
      "citations": 7,
      "downloads": 83,
      "id": 54,
      "proceedingsId": 4,
      "fullAbstract": "\n\t\t<p>Modern virtual machines for object-oriented languages such as Java HotSpot, Javascript V8 or Python PyPy reach high performance through just-in-time compilation techniques, involving on-the-fly optimization and deoptimization of the executed code. These techniques require a warm-up time for the virtual machine to collect information about the code it executes to be able to generate highly optimized code. This warm-up time required before reaching peak performance can be considerable and problematic. In this paper, we propose an approach, Sista (Speculative Inlining SmallTalk Architecture) to persist optimized code in a platform-independent representation as part of a snapshot. After explaining the overall approach, we show on a large set of benchmarks that the Sista virtual machine can reach peak performance almost immediately after start-up when using a snapshot where optimized code was persisted.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Applying Optimizations for Dynamically-typed Languages to Java",
      "url": "https://dl.acm.org/doi/10.1145/3132190.3132202",
      "doi": "10.1145/3132190.3132202",
      "authors": [
        "Matthias Grimmer",
        "Stefan Marr",
        "Mario Kahlhofer",
        "Christian Wimmer",
        "Thomas Würthinger",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2017",
      "pages": "pp 12–22",
      "shortAbstract": "<p>While Java is a statically-typed language, some of its features make it behave like a dynamically-typed language at run time. This includes Java's boxing of primitive values as well as generics, which rely on type erasure.</p> <p>This paper investigates how ...</p>",
      "citations": 4,
      "downloads": 150,
      "id": 55,
      "proceedingsId": 4,
      "fullAbstract": "\n\t\t<p>While Java is a statically-typed language, some of its features make it behave like a dynamically-typed language at run time. This includes Java's boxing of primitive values as well as generics, which rely on type erasure.</p> <p>This paper investigates how runtime technology for dynamically-typed languages such as JavaScript and Python can be used for Java bytecode. Using optimistic optimizations, we specialize bytecode instructions that access references in such a way, that they can handle primitive data directly and also specialize data structures in order to avoid boxing for primitive types. Our evaluation shows that these optimizations can be successfully applied to a statically-typed language such as Java and can also improve performance significantly. With this approach, we get an efficient implementation of Java's generics, avoid changes to the Java language, and maintain backwards compatibility, allowing existing code to benefit from our optimization transparently.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Introducing Scala-like function types into Java-TX",
      "url": "https://dl.acm.org/doi/10.1145/3132190.3132203",
      "doi": "10.1145/3132190.3132203",
      "authors": [
        "Martin Plümicke",
        "Andreas Stadelmeier"
      ],
      "monthYear": "September 2017",
      "pages": "pp 23–34",
      "shortAbstract": "<p>This paper considers the realisation of lambda expressions in Java 8 on the basis of a global type inference algorithm, which we have introduced in Java-TX. We demonstrate that the Java 8 approach has indeed some benefits but also a number of drawbacks. ...</p>",
      "citations": 0,
      "downloads": 64,
      "id": 56,
      "proceedingsId": 4,
      "fullAbstract": "\n\t\t<p>This paper considers the realisation of lambda expressions in Java 8 on the basis of a global type inference algorithm, which we have introduced in Java-TX. We demonstrate that the Java 8 approach has indeed some benefits but also a number of drawbacks. In order to eliminate the drawbacks, we take into consideration the approaches in a former experimental Java version (strawman approach) and in Scala. We show that an integration of these approaches eliminates the drawbacks without losing the benefits of the Java 8 approach. Additionally, we adapt our global type inference algorithm to this extended language.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Lenient Execution of C on a Java Virtual Machine: or: How I Learned to Stop Worrying and Run the Code",
      "url": "https://dl.acm.org/doi/10.1145/3132190.3132204",
      "doi": "10.1145/3132190.3132204",
      "authors": [
        "Manuel Rigger",
        "Roland Schatz",
        "Matthias Grimmer",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2017",
      "pages": "pp 35–47",
      "shortAbstract": "<p>Most C programs do not conform strictly to the C standard, and often show undefined behaviors, for instance, in the case of signed integer overflow. When compiled by non-optimizing compilers, such programs often behave as the programmer intended. ...</p>",
      "citations": 6,
      "downloads": 95,
      "id": 57,
      "proceedingsId": 4,
      "fullAbstract": "\n\t\t<p>Most C programs do not conform strictly to the C standard, and often show undefined behaviors, for instance, in the case of signed integer overflow. When compiled by non-optimizing compilers, such programs often behave as the programmer intended. However, optimizing compilers may exploit undefined semantics to achieve more aggressive optimizations, possibly breaking the code in the process. Analysis tools can help to find and fix such issues. Alternatively, a C dialect could be defined in which clear semantics are specified for frequently occurring program patterns with otherwise undefined behaviors. In this paper, we present Lenient C, a C dialect that specifies semantics for behaviors left open for interpretation in the standard. Specifying additional semantics enables programmers to make safe use of otherwise undefined patterns. We demonstrate how we implemented the dialect in Safe Sulong, a C interpreter with a dynamic compiler that runs on the JVM.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "DAMMP: A Distributed Actor Model for Mobile Platforms",
      "url": "https://dl.acm.org/doi/10.1145/3132190.3132205",
      "doi": "10.1145/3132190.3132205",
      "authors": [
        "Arghya Chatterjee",
        "Srđan Milaković",
        "Bing Xue",
        "Zoran Budimlić",
        "Vivek Sarkar"
      ],
      "monthYear": "September 2017",
      "pages": "pp 48–59",
      "shortAbstract": "<p>While mobile computing has seen a trend towards miniaturization and energy savings for a number of years, the available hardware parallelism in mobile devices has at the same time continued to increase. Overall, mobile devices remain resource ...</p><p></p><p></p>",
      "citations": 0,
      "downloads": 83,
      "id": 58,
      "proceedingsId": 4,
      "fullAbstract": "\n\t\t<p>While mobile computing has seen a trend towards miniaturization and energy savings for a number of years, the available hardware parallelism in mobile devices has at the same time continued to increase. Overall, mobile devices remain resource constrained on power consumption and thermal dissipation. Aggregating the computing capabilities of multiple mobile devices in a distributed and dynamic setting, opens the possibilities for performance improvements, longer aggregate battery life and novel dynamic and distributed applications.</p> <p>In this paper, we propose a Distributed Actor Model for Mobile Platforms (DAMMP), which includes a) a mobile extension to the actor-based Distributed Selector (DS) programming model, along with a new implementation for mobile Android devices, b) an extension to the DS programming model that enables the programmer to react and adapt to dynamic changes in device availability, c) an adaptive mobile-to-server and mobile-to-mobile computation offloading model and its implementation on the Android platform, and d) creation of a dynamic network of heterogeneous Android devices using both Wi-Fi Soft AP and Wi-Fi Direct's peer to peer (P2P) network.</p> <p>We evaluate the DAMMP framework under ideal thermally-controlled usage conditions to show promising scalability and performance, and analyze the communication overhead of both Wi-Fi and Wi-Fi Direct when used as the communication layer for DAMMP. We also evaluate the impact of adaptive offload on device-level thermal dissipation in more realistic usage scenarios, thereby demonstrating possibilities for thermal control and power management that can be achieved at the application level with a distributed actor model. To the best of our knowledge, this work is the first cross-platform distributed actor/selector runtime system that can span mobile devices and distributed servers.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Querying Invisible Objects: Supporting Data-Driven, Privacy-Preserving Distributed Applications",
      "url": "https://dl.acm.org/doi/10.1145/3132190.3132206",
      "doi": "10.1145/3132190.3132206",
      "authors": [
        "Yin Liu",
        "Zheng Song",
        "Eli Tilevich"
      ],
      "monthYear": "September 2017",
      "pages": "pp 60–72",
      "shortAbstract": "<p>When transferring sensitive data to a non-trusted party, end-users require that the data be kept private. Mobile and IoT application developers want to leverage the sensitive data to provide better user experience and intelligent services. Unfortunately,...</p>",
      "citations": 2,
      "downloads": 155,
      "id": 59,
      "proceedingsId": 4,
      "fullAbstract": "\n\t\t<p>When transferring sensitive data to a non-trusted party, end-users require that the data be kept private. Mobile and IoT application developers want to leverage the sensitive data to provide better user experience and intelligent services. Unfortunately, existing programming abstractions make it impossible to reconcile these two seemingly conflicting objectives. In this paper, we present a novel programming mechanism for distributed managed execution environments that hides sensitive user data, while enabling developers to build powerful and intelligent applications, driven by the properties of the sensitive data. Specifically, the sensitive data is never revealed to clients, being protected by the runtime system. Our abstractions provide declarative and configurable data query interfaces, enforced by a lightweight distributed runtime system. Developers define when and how clients can query the sensitive data's properties (i.e., how long the data remains accessible, how many times its properties can be queried, which data query methods apply, etc.). Based on our evaluation, we argue that integrating our novel mechanism with the Java Virtual Machine (JVM) can address some of the most pertinent privacy problems of IoT and mobile applications.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Experiences with Building Domain-Specific Compilation Plugins in Graal",
      "url": "https://dl.acm.org/doi/10.1145/3132190.3132207",
      "doi": "10.1145/3132190.3132207",
      "authors": [
        "Colin Barrett",
        "Christos Kotselidis",
        "Foivos S. Zakkak",
        "Nikos Foutris",
        "Mikel Luján"
      ],
      "monthYear": "September 2017",
      "pages": "pp 73–84",
      "shortAbstract": "<p>In this paper, we describe our experiences in co-designing a domain-specific compilation stack. Our motivation stems from the missed optimization opportunities we observed while implementing a computer vision library in Java. To tackle the performance ...</p><p></p>",
      "citations": 1,
      "downloads": 88,
      "id": 60,
      "proceedingsId": 4,
      "fullAbstract": "\n\t\t<p>In this paper, we describe our experiences in co-designing a domain-specific compilation stack. Our motivation stems from the missed optimization opportunities we observed while implementing a computer vision library in Java. To tackle the performance shortcomings, we developed Indigo, a computer vision API co-designed with a compilation plugin for optimizing computer vision applications.</p> <p>Indigo exploits the extensible nature of the Graal compiler which provides invocation plugins, that replace methods with dedicated nodes, and generates machine code compatible with both the Java Virtual Machine (JVM) and the SIMD hardware unit. Our approach improves performance by up to 66.75× when compared to pure Java implementations and by up to 2.75× when compared to the original C++ implementation. These performance improvements are the result of low-level concurrency, idiomatic implementation of algorithms, and by keeping temporary objects in the wider vector unit registers.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Towards an Efficient Pauseless Java GC with Selective HTM-Based Access Barriers",
      "url": "https://dl.acm.org/doi/10.1145/3132190.3132208",
      "doi": "10.1145/3132190.3132208",
      "authors": [
        "Maria Carpen-Amarie",
        "Yaroslav Hayduk",
        "Pascal Felber",
        "Christof Fetzer",
        "Gaël Thomas",
        "Dave Dice"
      ],
      "monthYear": "September 2017",
      "pages": "pp 85–91",
      "shortAbstract": "<p>The garbage collector (GC) is a critical component of any managed runtime environment (MRE), such as the Java virtual machine. While the main goal of the GC is to simplify and automate memory management, it may have a negative impact on the application ...</p>",
      "citations": 1,
      "downloads": 67,
      "id": 61,
      "proceedingsId": 4,
      "fullAbstract": "\n\t\t<p>The garbage collector (GC) is a critical component of any managed runtime environment (MRE), such as the Java virtual machine. While the main goal of the GC is to simplify and automate memory management, it may have a negative impact on the application performance, especially on multi-core systems. This is typically due to stop-the-world pauses, i.e., intervals for which the application threads are blocked during the collection. Existing approaches to concurrent GCs allow the application threads to perform at the same time as the GC at the expense of throughput and simplicity. In this paper we build upon an existing pauseless transactional GC algorithm and design an important optimization that would significantly increase its throughput. More precisely, we devise selective access barriers, that define multiple paths based on the state of the garbage collector. Preliminary evaluation of the selective barriers shows up to 93% improvement over the initial transactional barriers in the worst case scenario. We estimate the performance of a pauseless GC having selective transactional barriers and find it to be on par with Java's concurrent collector.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Trace Register Allocation Policies: Compile-time vs. Performance Trade-offs",
      "url": "https://dl.acm.org/doi/10.1145/3132190.3132209",
      "doi": "10.1145/3132190.3132209",
      "authors": [
        "Josef Eisl",
        "Stefan Marr",
        "Thomas Würthinger",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2017",
      "pages": "pp 92–104",
      "shortAbstract": "<p>Register allocation is an integral part of compilation, regardless of whether a compiler aims for fast compilation or optimal code quality. State-of-the-art dynamic compilers often use global register allocation approaches such as linear scan. Recent ...</p><p></p><p></p><p></p><p></p>",
      "citations": 4,
      "downloads": 132,
      "id": 62,
      "proceedingsId": 4,
      "fullAbstract": "\n\t\t<p>Register allocation is an integral part of compilation, regardless of whether a compiler aims for fast compilation or optimal code quality. State-of-the-art dynamic compilers often use global register allocation approaches such as linear scan. Recent results suggest that non-global trace-based register allocation approaches can compete with global approaches in terms of allocation quality. Instead of processing the whole compilation unit (i.e., method) at once, a trace-based register allocator divides the problem into linear code segments, called traces.</p> <p>In this work, we present a register allocation framework that can exploit the additional flexibility of traces to select different allocation strategies based on the characteristics of a trace. This provides us with fine-grained control over the trade-off between compile time and peak performance in a just-in-time compiler.</p> <p>Our framework features three allocation strategies: a linear-scan-based approach that achieves good code quality, a single-pass bottom-up strategy that aims for short allocation times, and an allocator for trivial traces.</p> <p>To demonstrate the flexibility of the framework, we select 8 allocation policies and show their impact on compile time and peak performance. This approach can reduce allocation time by 7%--43% at a peak performance penalty of about 1%--11% on average.</p> <p>For systems that do not focus on peak performance, our approach allows to adjust the time spent for register allocation, and therefore the overall compilation time, thus finding the optimal balance between compile time and peak performance according to an application's requirements.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Integrating Profile Caching into the HotSpot Multi-Tier Compilation System",
      "url": "https://dl.acm.org/doi/10.1145/3132190.3132210",
      "doi": "10.1145/3132190.3132210",
      "authors": [
        "Zoltan Majo",
        "Tobias Hartmann",
        "Marcel Mohler",
        "Thomas R. Gross"
      ],
      "monthYear": "September 2017",
      "pages": "pp 105–118",
      "shortAbstract": "<p>The Java®HotSpot Virtual Machine includes a multi-tier compilation system that may invoke a compiler at any time. Lower tiers instrument the program to gather information for the highly optimizing compiler at the top tier, and this compiler bases its ...</p><p></p>",
      "citations": 2,
      "downloads": 109,
      "id": 63,
      "proceedingsId": 4,
      "fullAbstract": "\n\t\t<p>The Java®HotSpot Virtual Machine includes a multi-tier compilation system that may invoke a compiler at any time. Lower tiers instrument the program to gather information for the highly optimizing compiler at the top tier, and this compiler bases its optimizations on these profiles. But if the assumptions made by the top-tier compiler are proven wrong (e.g., because the profile does not cover all execution paths), the method is deoptimized: the code generated for the method is discarded and the method is then executed at Tier 0 again. Eventually, after profile information has been gathered, the method is recompiled at the top tier again (this time with less-optimistic assumptions). Users of the system experience such deoptimization cycles (discard, profile, compile) as performance fluctuations and potentially as variations in the system's responsiveness. Unpredictable performance however is problematic in many time-critical environments even if the system is not a hard real-time system.</p> <p>A profile cache captures the profile of earlier executions. When the application is executed again, with a fresh VM, the top tier (highly optimizing) compiler can base its decisions on a profile that reflects prior executions and not just the recent history observed during this run. We report in this paper the design and effectiveness of a profile cache for Java applications which is implemented and evaluated as part of the multi-tier compilation system of the HotSpot Java Virtual Machine in OpenJDK version 9. For a set of benchmarks, profile caching reduces the number of (re)compilations by up to 23%, the number of deoptimizations by up to 90%, and thus improves performance predictability.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Deeply Reifying Running Code for Constructing a Domain-Specific Language",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972219",
      "doi": "10.1145/2972206.2972219",
      "authors": [
        "Shigeru Chiba",
        "YungYu Zhuang",
        "Maximilian Scherr"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 1",
      "shortAbstract": "<p>This paper presents deep reification, which is a language mechanism for reflective computing. It reifies a self-contained partial snapshot of the current execution environment. The snapshot contains not only data but also code and type definitions. This ...</p>",
      "citations": 5,
      "downloads": 127,
      "id": 64,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>This paper presents deep reification, which is a language mechanism for reflective computing. It reifies a self-contained partial snapshot of the current execution environment. The snapshot contains not only data but also code and type definitions. This mechanism can be used as a common component of execution systems of embedded domain-specific languages (DSLs). Unlike typical implementations of embedded DSLs, the mechanism enables a DSL to borrow the host-language's syntax yet execute under a different semantics on different platforms from the host language's. DSL implementation can allow programmers to first construct a function closure as DSL code, reify a snapshot necessary for executing the function closure, transform the code in the snapshot into a program for a target platform, and finally execute the program. As a prototype system of deep reification, we have implemented Bytespresso for Java. This paper shows Bytespresso and also Bytespresso-C, our DSL built on top of Bytespresso. The target of this DSL is numerical computing on cluster computers and GPUs.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "DiSquawk: 512 cores, 512 memories, 1 JVM",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972212",
      "doi": "10.1145/2972206.2972212",
      "authors": [
        "Foivos S. Zakkak",
        "Polyvios Pratikakis"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 2",
      "shortAbstract": "<p>Trying to cope with the constantly growing number of cores per processor, hardware architects are experimenting with modular non cache coherent architectures. Such architectures delegate the memory coherency to the software. On the contrary, high ...</p><p></p>",
      "citations": 1,
      "downloads": 75,
      "id": 65,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>Trying to cope with the constantly growing number of cores per processor, hardware architects are experimenting with modular non cache coherent architectures. Such architectures delegate the memory coherency to the software. On the contrary, high productivity languages like Java are designed to abstract away the hardware details and allow developers to focus on the implementation of their algorithm. Such programming languages rely on a process virtual machine to perform the necessary operations to implement the corresponding memory model. Arguing, however, about the correctness of such implementations is not trivial.</p> <p>This paper presents our implementation of the Java Memory Model in a Java Virtual Machine targeting a 512-core non cache coherent memory architecture. We shortly discuss design decisions and present evaluation results demonstrating that our implementation scales with the number of cores, up to 512 cores. We model our implementation as the operational semantics of a Java Core Calculus that we extend with synchronization actions, and prove its adherence to the Java Memory Model.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "A Distributed Selectors Runtime System for Java Applications",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972215",
      "doi": "10.1145/2972206.2972215",
      "authors": [
        "Arghya Chatterjee",
        "Branko Gvoka",
        "Bing Xue",
        "Zoran Budimlic",
        "Shams Imam",
        "Vivek Sarkar"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 3",
      "shortAbstract": "<p>The demand for portable mainstream programming models supporting scalable, reactive and versatile distributed computing is growing dramatically with the proliferation of manycore/heterogeneous processors on portable devices and cloud computing clusters ...</p><p></p><p></p>",
      "citations": 4,
      "downloads": 107,
      "id": 66,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>The demand for portable mainstream programming models supporting scalable, reactive and versatile distributed computing is growing dramatically with the proliferation of manycore/heterogeneous processors on portable devices and cloud computing clusters that can be elastically and dynamically allocated. With such changes, distributed software systems and applications are shifting towards service oriented architectures (SOA) that consist of largely decoupled, dynamically replaceable components and connected via loosely coupled, interactive networks that may exhibit more complex coordination and synchronization patterns.</p> <p>In this paper, we propose the Distributed Selector (DS) model, to address the aforementioned requirements via a simple easy-to-use API. Our implementation of this model runs on distributed JVMs, and features automated bootstrap and global termination. We focus on the Selector Model (a generalization of the actor model) as a foundation for creating distributed programs and introduce a unified runtime system that supports both shared memory and distributed multi-node execution of such programs. The multiple guarded mailboxes, a unique and novel property of Selectors, enable the programmer to easily specify coordination patterns that are strictly more general than those supported by the Actor model.</p> <p>We evaluate the performance of our selector-based distributed implementation using benchmarks from the Savina benchmark suite [13]. Our results show promising scalability performance for various message exchange patterns. We also demonstrate high programming productivity arising from high-level abstraction and location transparency in the HJ Distributed Selector Runtime library (as evidenced by minimal differences between single-node and multi-node implementations of a selector-based application), as well as the contribution of automated system bootstrap and global termination capabilities.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Efficient Memory Traces with Full Pointer Information",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972220",
      "doi": "10.1145/2972206.2972220",
      "authors": [
        "Philipp Lengauer",
        "Verena Bitto",
        "Stefan Fitzek",
        "Markus Weninger",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 4",
      "shortAbstract": "<p>Tracing objects and their references is paramount for tracking down memory-related performance problems. In this paper, we present a novel approach for recording references between objects at minimal run-time overhead. Extending our memory monitoring ...</p>",
      "citations": 10,
      "downloads": 162,
      "id": 67,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>Tracing objects and their references is paramount for tracking down memory-related performance problems. In this paper, we present a novel approach for recording references between objects at minimal run-time overhead. Extending our memory monitoring tool AntTracks with pointer information allows for a complete reconstruction of the heap before and after garbage collections as well as for an offline analysis of all garbage collection decisions made. Additionally, we describe several trace optimization techniques, such as exploiting VM-internal information, to reduce the tracing overhead even further. We evaluate our approach by means of the DaCapo benchmark suite and a selection of memory-intensive benchmarks from the DaCapoScala benchmark suite and the SPECjvm benchmark suite.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Extraction-Based Regression Test Selection",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972224",
      "doi": "10.1145/2972206.2972224",
      "authors": [
        "Jesper Öqvist",
        "Görel Hedin",
        "Boris Magnusson"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 5",
      "shortAbstract": "<p>Frequent regression testing is a core activity in agile software development, but large test suites can lead to long test running times, hampering agility. By safe RTS (Regression Test Selection) techniques, a subset of the tests can be identified that ...</p>",
      "citations": 6,
      "downloads": 225,
      "id": 68,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>Frequent regression testing is a core activity in agile software development, but large test suites can lead to long test running times, hampering agility. By safe RTS (Regression Test Selection) techniques, a subset of the tests can be identified that cover all tests that can change result since the last run. To pay off in practice, the RTS overhead must be low. Most existing RTS techniques are based on dynamic coverage analysis, making the overhead related to the tests run. We present Extraction-Based RTS, a new safe RTS technique which uses a fast static analysis with very low overhead, related to the size of the modification rather than to the tests run. The method is suitable for program-driven testing, commonly used in agile development, where each test is a piece of code that uses parts of the system under test. We have implemented the method for Java, and bench-marked it on a number of open source projects, showing that it pays off substantially in practice.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Inference and Checking of Object Immutability",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972208",
      "doi": "10.1145/2972206.2972208",
      "authors": [
        "Ana Milanova",
        "Yao Dong"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 6",
      "shortAbstract": "<p>Reference immutability guarantees that a reference is not used to modify the referenced object. It is well-understood and there are several scalable inference systems. Object immutability, a stronger immutability guarantee, guarantees that an object is ...</p><p></p><p></p><p></p>",
      "citations": 4,
      "downloads": 103,
      "id": 69,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>Reference immutability guarantees that a reference is not used to modify the referenced object. It is well-understood and there are several scalable inference systems. Object immutability, a stronger immutability guarantee, guarantees that an object is not modified. Unfortunately, object immutability is not as well-understood; specifically, we are unaware of an inference system that infers object immutability across large Java programs and libraries.</p> <p>It is tempting to use reference immutability to reason about object immutability. However, representation exposure and object initialization pose significant challenges.</p> <p>In this paper we present a novel type system and a corresponding inference analysis. We leverage reference immutability to infer object immutability overcoming the challenges due to representation exposure and object initialization.</p> <p>We have implemented our object immutability system for Java. Evaluation on the standard Dacapo benchmarks demonstrates precision and scalability. Nearly 40% of all static objects are inferred immutable. Analysis completes in under 2 minutes on all benchmarks.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Integrating Asynchronous Task Parallelism and Data-centric Atomicity",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972214",
      "doi": "10.1145/2972206.2972214",
      "authors": [
        "Vivek Kumar",
        "Julian Dolby",
        "Stephen M. Blackburn"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 7",
      "shortAbstract": "<p>Processor design has turned toward parallelism and heterogeneous cores to achieve performance and energy efficiency. Developers find high-level languages attractive as they use abstraction to offer productivity and portability over these hardware ...</p><p></p>",
      "citations": 2,
      "downloads": 81,
      "id": 70,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>Processor design has turned toward parallelism and heterogeneous cores to achieve performance and energy efficiency. Developers find high-level languages attractive as they use abstraction to offer productivity and portability over these hardware complexities. Over the past few decades, researchers have developed increasingly advanced mechanisms to deliver performance despite the overheads naturally imposed by this abstraction. Recent work has demonstrated that such mechanisms can be exploited to attack overheads that arise in emerging high-level languages, which provide strong abstractions over parallelism. However, current implementation of existing popular high-level languages, such as Java, offer little by way of abstractions that allow the developer to achieve performance in the face of extensive hardware parallelism.</p> <p>In this paper, we present a small set of extensions to the Java programming language that aims to achieve both high performance and high productivity with minimal programmer effort. We incorporate ideas from languages like X10 and AJ to develop five annotations in Java for achieving asynchronous task parallelism and data-centric concurrency control. These annotations allow the use of a highly efficient implementation of a work-stealing scheduler for task parallelism. We evaluate our proposal by refactoring classes from a number of existing multithreaded open source projects to use our new annotations. Our results suggest that these annotations significantly reduce the programming effort while achieving performance improvements up to 30% compared to conventional approaches.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "JCrypt: Towards Computation over Encrypted Data",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972209",
      "doi": "10.1145/2972206.2972209",
      "authors": [
        "Yao Dong",
        "Ana Milanova",
        "Julian Dolby"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 8",
      "shortAbstract": "<p>Cloud computing allows clients to upload data and computation to untrusted servers, which leads to potential violations to the confidentiality of client data. We propose JCrypt, a static program analysis which transforms a Java program into an ...</p>",
      "citations": 12,
      "downloads": 126,
      "id": 71,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>Cloud computing allows clients to upload data and computation to untrusted servers, which leads to potential violations to the confidentiality of client data. We propose JCrypt, a static program analysis which transforms a Java program into an equivalent one, so that it performs computation over encrypted data and preserves data confidentiality. JCrypt minimizes computation over encrypted data. It consists of two stages. The first stage is a type-based information flow analysis which partitions the program so that only sensitive parts need to be encrypted. The second stage is an inter-procedural data-flow analysis, similar to the classical Available Expressions. It deduces the appropriate encryption scheme for sensitive variables. We implemented JCrypt for Java and showed that our analysis is effective and practical using five benchmark suites. JCrypt encrypts a significantly larger percentage of benchmarks compared to MrCrypt, the closest related work.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Multi-tier Data Synchronization Based on an Optimized Concurrent Linked-list",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972213",
      "doi": "10.1145/2972206.2972213",
      "authors": [
        "Bing Yang",
        "Kenneth B. Kent",
        "Eric Aubanel",
        "Angela Lin",
        "Tobi Ajila"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 9",
      "shortAbstract": "<p>In this paper, we present a multi-tier data synchronization approach, MultitierPackedSync, based upon the packed object data model (PackedObject), introduced in the IBM J9 Virtual Machine as an experimental enhancement. The packed object data model ...</p>",
      "citations": 1,
      "downloads": 80,
      "id": 72,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>In this paper, we present a multi-tier data synchronization approach, MultitierPackedSync, based upon the packed object data model (PackedObject), introduced in the IBM J9 Virtual Machine as an experimental enhancement. The packed object data model organizes data in a multi-tier manner (like C structs) in which the object's data is nested in its enclosing PackedObject instead of being pointed to by an object reference as in the traditional Java object data model. We developed the MultitierPackedSync approach to multi-tier data synchronization based on a concurrent linked-list, which optimizes the state-of-the-art linked-list algorithm (lazy-list) and applies it to multi-tier PackedObject synchronization. This approach allows multiple threads to lock on a multi-tier PackedObject from different tiers and maintain consistency. We also implemented the Lazy_MultitierPackedSync technique based upon the lazy-list algorithm without our optimization to show the performance gain of MultitierPackedSync. Compared with our previous work (PackedSync), Lazy_MultitierPackedSync and the Java built-in synchronization in single-granularity locking, the performance of MultitierPackedSync is a significant improvement although it is relatively slower than the Java built-in synchronization in the fine-grained locking scenario when the number of threads grows. For multi-granularity locking, we compared MultitierPackedSync with the Lazy_MultitierPackedSync. The results show our optimized approach MultitierPackedSync improves the overall performance considerably.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Preexistence and concrete type analysis in the context of multiple inheritance",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972207",
      "doi": "10.1145/2972206.2972207",
      "authors": [
        "Roland Ducournau",
        "Julien Pagès",
        "Jean Privat"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 10",
      "shortAbstract": "<p>In the framework of just-in-time compilation, preexistence is a property which asserts that the receiver of a given call site has been instantiated before the current invocation of the considered method [5]. Hence, preexistence is a mean to avoid such ...</p>",
      "citations": 0,
      "downloads": 62,
      "id": 73,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>In the framework of just-in-time compilation, preexistence is a property which asserts that the receiver of a given call site has been instantiated before the current invocation of the considered method [5]. Hence, preexistence is a mean to avoid such low-level repair techniques as on-stack replacement or code patching, when a method must be recompiled during its own activation, because a previous optimization such as devirtualization or inlining has been invalidated by a new class loading. In the original proposition, preexistence considers only values, its analysis is purely intra-procedural, and its result is immutable. In this paper, we reconsider all three aspects: we focus on types instead of values, especially on concrete types, and we accept a limited form of inter-procedural analysis, whose results can be, marginally, mutable. Moreover, while the original proposition considered only method invocation, we place ourselves in the context of full multiple-inheritance, where attribute accesses need to be optimized, too. Overall, we propose a static analysis at runtime, combining concrete types and preexistence. The former allow for more optimizations, while the latter provides the condition for applying these optimizations without any need for repair. We experimented the approach in the prototype of a virtual machine for the Nit language, and the results are really encouraging. In spite of the focus on multiple inheritance, and the experiment setting, this proposal could be applied to most languages with static typing and runtime systems like the Java virtual machine.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Prioritizing Regression Tests for Desktop and Web-Applications based on the Execution Frequency of Modified Code",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972222",
      "doi": "10.1145/2972206.2972222",
      "authors": [
        "Matthias Hirzel",
        "Jonathan Immanuel Brachthäuser",
        "Herbert Klaeren"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 11",
      "shortAbstract": "<p>Regression testing can be very time expensive when running all available test cases. Test prioritization seeks to find faults early by reordering tests. Existing techniques decide in which order tests should be run based on coverage data, knowledge of ...</p>",
      "citations": 0,
      "downloads": 123,
      "id": 74,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>Regression testing can be very time expensive when running all available test cases. Test prioritization seeks to find faults early by reordering tests. Existing techniques decide in which order tests should be run based on coverage data, knowledge of code changes, historical data of prior test execution or a combination of them. Others postpone tests if similar ones are already selected for early execution. However, these approaches do not take into account that tests which appear similar still might explore different parts of the application's state space and thus can result in different test outcome. Approaches based on structural coverage or on historical data might ignore small tests focusing on behavior that rarely changes. In this paper, we present a novel prioritization technique that is based on the frequencies with which modified code parts are executed by the tests. Our technique assumes that multiple executions of a modified code part (under different contexts) have a higher chance to reveal faults than a single execution of this code. For this purpose, we use both the output of regression test selection as well as test traces obtained during test development. We propose multiple variants of our technique, including a feedback mechanism to optimize the prioritization order dynamically, and compare them in an evaluation of Java-based applications to existing approaches using the standard APFD metric. The results show that our technique is highly competitive.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Real-Time Program-Specific Phase Change Detection for Java Programs",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972221",
      "doi": "10.1145/2972206.2972221",
      "authors": [
        "Meng-Chieh Chiu",
        "Benjamin Marlin",
        "Eliot Moss"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 12",
      "shortAbstract": "<p>It is well-known that programs tend to have multiple phases in their execution. Because phases have impact on micro-architectural features such as caches and branch predictors, they are relevant to program performance Xian et al. [2007], Roh et al. [...</p>",
      "citations": 1,
      "downloads": 285,
      "id": 75,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>It is well-known that programs tend to have multiple phases in their execution. Because phases have impact on micro-architectural features such as caches and branch predictors, they are relevant to program performance Xian et al. [2007], Roh et al. [2009], Gu and Verbrugge [2008] and energy consumption. They are also relevant to detecting whether a program is executing as expected or is encountering unusual or exceptional conditions, a software engineering and program monitoring concern Peleg and Mendelson [2007], Singer and Kirkham [2008], Pirzadeh et al. [2011], Benomar et al. [2014]. We offer here a method for real-time phase change detection in Java programs. After applying a training protocol to a program of interest, our method can detect phase changes at run time for that program with good precision and recall (compared with a \"ground truth\" definition of phases) and with small performance impact (average less than 2%). We also offer improved methodology for evaluating phase change detection mechanisms. In sum, our approach offers the first known implementation of real-time phase detection for Java programs.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Shenandoah: An open-source concurrent compacting garbage collector for OpenJDK",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972210",
      "doi": "10.1145/2972206.2972210",
      "authors": [
        "Christine H. Flood",
        "Roman Kennke",
        "Andrew Dinn",
        "Andrew Haley",
        "Roland Westrelin"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 13",
      "shortAbstract": "<p>Shenandoah is an open-source region-based low-pause parallel and concurrent garbage collection (GC) algorithm targeting large heap applications. Snapshot At the Beginning Concurrent Marking and Brooks-style indirection pointer concurrent compaction ...</p><p></p>",
      "citations": 23,
      "downloads": 356,
      "id": 76,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>Shenandoah is an open-source region-based low-pause parallel and concurrent garbage collection (GC) algorithm targeting large heap applications. Snapshot At the Beginning Concurrent Marking and Brooks-style indirection pointer concurrent compaction enable significantly shorter GC pauses with durations that are independent of the application's live data size. Our implementation of Shenandoah in OpenJDK allows us to do comparison testing with mature production quality GC algorithms.</p> <p>Modern machines have more memory and more processors than ever before. Service Level Agreement (SLA) applications guarantee response times of 10-500ms. In order to meet the lower end of that goal we need garbage collection algorithms which are efficient enough to allow programs to run in the available memory, but also optimized to never interrupt the running program for more than a handful of milliseconds. Shenandoah is an open-source low-pause time collector for OpenJDK designed to move closer to those goals.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Trace-based Register Allocation in a JIT Compiler",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972211",
      "doi": "10.1145/2972206.2972211",
      "authors": [
        "Josef Eisl",
        "Matthias Grimmer",
        "Doug Simon",
        "Thomas Würthinger",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 14",
      "shortAbstract": "<p>State-of-the-art dynamic compilers often use global approaches, like Linear Scan or Graph Coloring, for register allocation. These algorithms consider the complete compilation unit for allocation, which increases the complexity of the implementation (...</p><p></p>",
      "citations": 7,
      "downloads": 321,
      "id": 77,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>State-of-the-art dynamic compilers often use global approaches, like Linear Scan or Graph Coloring, for register allocation. These algorithms consider the complete compilation unit for allocation, which increases the complexity of the implementation (e.g., support for lifetime holes in Linear Scan) and potentially also affects compilation time. We propose a novel non-global algorithm, which splits a compilation unit into traces based on profiling feedback and subsequently performs register allocation within each trace individually. Traces reduce the problem size to a single linear code segment, which simplifies the problem a register allocator needs to solve. Additionally, we can apply different register allocation algorithms to each trace. We show that this non-global approach can achieve results competitive to global register allocation.</p> <p>We present an implementation of Trace Register Allocation based on the Graal VM and show an evaluation for common Java benchmarks. We demonstrate that performance of this non-global approach is within 3% (on AMD64) and 1% (on SPARC) of global Linear Scan register allocation.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Dependency Safety for Java: Implementing Failboxes",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972216",
      "doi": "10.1145/2972206.2972216",
      "authors": [
        "Dragan Bošnački",
        "Mark van den Brand",
        "Philippe Denissen",
        "Cornelis Huizing",
        "Bart Jacobs",
        "Ruurd Kuiper",
        "Anton Wijs",
        "Maciej Wiłkowski",
        "Dan Zhang"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 15",
      "shortAbstract": "<p>Exception mechanisms help to ensure that a program satisfies the important robustness criterion of dependency safety: if an operation fails, no code that depends on the operation's successful completion is executed anymore nor will wait for the ...</p>",
      "citations": 1,
      "downloads": 95,
      "id": 78,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>Exception mechanisms help to ensure that a program satisfies the important robustness criterion of dependency safety: if an operation fails, no code that depends on the operation's successful completion is executed anymore nor will wait for the completion. However, the exception handling mechanisms available in languages like Java do not provide a structured way to achieve dependency safety. The language extension failbox provides dependency safety for Java in a compositional manner. So far, there only exists an implementation of failbox in Scala. It requires the assumption of absence of asynchronous exceptions inside the failbox code. In this paper, we are the first to provide an implementation without the above mentioned assumption, this time in Java. First, we present and discuss a direct reimplementation of failbox in Java that is still restricted. Then, we show that using uncaught exception handlers the earlier assumption can be essentially weakened to only concern code before setting the thread handler. Finally, we provide an implementation using the Java native interface that completely removes the assumption.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Jbase: embedding Java expressions in Xtext DSLs",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972217",
      "doi": "10.1145/2972206.2972217",
      "authors": [
        "Lorenzo Bettini"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 16",
      "shortAbstract": "<p>In this paper we present Jbase, an expression language that can be used to introduce Java expressions and statements in Xtext DSLs. Jbase extends and customizes Xbase, the Java-like expression framework shipped with Xtext. Although Xbase already ...</p><p></p>",
      "citations": 0,
      "downloads": 141,
      "id": 79,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>In this paper we present Jbase, an expression language that can be used to introduce Java expressions and statements in Xtext DSLs. Jbase extends and customizes Xbase, the Java-like expression framework shipped with Xtext. Although Xbase already provides a complete integration with the Java type system and with all the existing Java libraries, its expression language is only Java-like, and this limits its adoption in contexts where Java-like is not enough.</p> <p>Jbase, built on top of Xtext/Xbase, offers all the powerful features of these frameworks and provides Java syntax, and the adaption of the Xbase type system and code generator accordingly. It also keeps the simplicity of Xbase, so that, by using Jbase, it is straightforward to introduce Java expressions and statements into an Xtext DSL. We believe that Jbase is a valuable tool for embedding Java expressions in a DSL, and also for extending Java expression syntax in extensions to Java.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "LLJava: Minimalist Structured Programming on the Java Virtual Machine [Work-in-Progress Research Paper]",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972218",
      "doi": "10.1145/2972206.2972218",
      "authors": [
        "Baltasar Trancón y Widemann",
        "Markus Lepper"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 17",
      "shortAbstract": "<p>There is a wide gap in abstraction level between Java source code and JVM bytecode. Many important software-related tasks, such as specification and implementation of code synthesis procedures, code inspection, software understanding and teaching, can ...</p>",
      "citations": 2,
      "downloads": 67,
      "id": 80,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>There is a wide gap in abstraction level between Java source code and JVM bytecode. Many important software-related tasks, such as specification and implementation of code synthesis procedures, code inspection, software understanding and teaching, can benefit from an adequate, intermediate level of abstraction. Available bytecode assembly/disassembly tools are ad-hoc and fall short of the requirements regarding compositionality and clarity. We report on the design and implementation of the LLJava language that bridges the gap, based on careful analysis of bytecode information and rigorous design.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Maintaining Unit Tests During Refactoring",
      "url": "https://dl.acm.org/doi/10.1145/2972206.2972223",
      "doi": "10.1145/2972206.2972223",
      "authors": [
        "Harrie Passier",
        "Lex Bijlsma",
        "Christoph Bockisch"
      ],
      "monthYear": "August 2016",
      "pages": "Article No.: 18",
      "shortAbstract": "<p>The meaning of source code is often described by unit tests, as is for example the case in Test-Driven Software Development. Test-driven development is a principle in software engineering that requires developers to write tests for each method before ...</p><p></p>",
      "citations": 6,
      "downloads": 172,
      "id": 81,
      "proceedingsId": 5,
      "fullAbstract": "\n\t\t<p>The meaning of source code is often described by unit tests, as is for example the case in Test-Driven Software Development. Test-driven development is a principle in software engineering that requires developers to write tests for each method before implementing the method itself. This ensures that for (at least) all public methods tests exist. When performing a refactoring, existing code is changed or restructured according to a predefined scheme. After a refactoring is applied, the alignment between the structure of source code and corresponding unit tests can be broken.</p> <p>In this paper we describe different ways in which refactorings can impact the API coverage of unit tests. We present our approach of tracking the modifications made by refactorings, analyzing their influence on the existing test suite and giving advice to developers on how to update the test suite to migrate it. For example, tests may need to be moved or new tests developed in case a refactoring introduced new public methods. Our approach is applicable to all refactorings. We conclude this paper by discussing the potential of the presented approach and of the preliminary tool support in the Eclipse IDE.</p>\n\t"
    },
    {
      "type": "invited-talk",
      "title": "JML: Expressive Contracts, Specification Inheritance, and Behavioral Subtyping",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2817926",
      "doi": "10.1145/2807426.2817926",
      "authors": [
        "Gary T. Leavens"
      ],
      "monthYear": "September 2015",
      "pages": "pp 1",
      "shortAbstract": "<p>JML, the Java Modeling Language, is a formal specification language tailored to the specification of sequential Java classes and interfaces. It features contracts in the style of design by contract (as in Eiffel), as well as more sophisticated features ...</p><p></p><p></p>",
      "citations": 3,
      "downloads": 100,
      "id": 82,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>JML, the Java Modeling Language, is a formal specification language tailored to the specification of sequential Java classes and interfaces. It features contracts in the style of design by contract (as in Eiffel), as well as more sophisticated features that allow it to be used with a variety of tools from dynamic assertion checking to static verification. The talk will explain JML using some small examples.</p> <p>JML also features a notion of \"specification inheritance,\" which forces all subtypes to be \"behavioral subtypes.\" Behavioral subtyping allows client code to validly reason about objects using \"supertype abstraction\"; for example, when calling a method on an object, the specification for that method in the object's static type can be used, even though the method call may dynamically dispatch to an overriding method in some subtype. Specification inheritance makes this valid by forcing each such overriding method to obey the specification of that method given in each of its supertypes. Specification inheritance, and thus supertype abstraction, also apply to JML's invariants, history constraints, and initially clauses. These features make reasoning about object-oriented programs modular.</p> <p>Work on JML has been supported in part by NSF grants CCF0916350, CCF0916715, CCF1017262, and CNS1228695.</p>\n\t"
    },
    {
      "type": "invited-talk",
      "title": "Java and Distributed Systems: Observations, Experiences, and ... a Wish List",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2817927",
      "doi": "10.1145/2807426.2817927",
      "authors": [
        "Niranjan Suri"
      ],
      "monthYear": "September 2015",
      "pages": "pp 1",
      "shortAbstract": "<p>When Java was introduced to the world at large 20 years ago, it brought many interesting features and capabilities into the mainstream computing environment. A Virtual Machine based approach with a just-in-time compiler that supported sandboxing, ...</p><p></p>",
      "citations": 0,
      "downloads": 50,
      "id": 83,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>When Java was introduced to the world at large 20 years ago, it brought many interesting features and capabilities into the mainstream computing environment. A Virtual Machine based approach with a just-in-time compiler that supported sandboxing, dynamic class loading, and introspection enabled a number of novel and innovative network-based applications to be developed. While many of these capabilities existed in some fashion in other prototype and experimental languages, the combination of all of them in a popular general purpose language opened up the possibility of building real systems that could leverage these capabilities. Applets, Jini, JXTA, and many other innovative concepts were introduced over the course of time, building on top of the basic capabilities of Java.</p> <p>This talk will present some personal experiences with using Java in distributed computing environments ranging from mobile software agents to distributed resource sharing to process integrated mechanisms. The basis for many of these capabilities is the Aroma Virtual Machine, a custom Java compatible VM with state capture, migration, and resource control capabilities. Motivations behind the Aroma VM will be discussed, along with design choices and some results. Finally, the talk will discuss a wish list of features that would be nice to have in future versions of Java to enable many more novel applications to be developed.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "HJ-OpenCL: Reducing the Gap Between the JVM and Accelerators",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807427",
      "doi": "10.1145/2807426.2807427",
      "authors": [
        "Max Grossman",
        "Shams Imam",
        "Vivek Sarkar"
      ],
      "monthYear": "September 2015",
      "pages": "pp 2–15",
      "shortAbstract": "<p>Recently there has been increasing interest in supporting execution of Java Virtual Machine (JVM) applications on accelerator architectures, such as GPUs. Unfortunately, there is a large gap between the features of the JVM and those commonly supported ...</p><p></p><p></p>",
      "citations": 2,
      "downloads": 150,
      "id": 84,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>Recently there has been increasing interest in supporting execution of Java Virtual Machine (JVM) applications on accelerator architectures, such as GPUs. Unfortunately, there is a large gap between the features of the JVM and those commonly supported by accelerators. Examples of important JVM features include exceptions, dynamic memory allocation, use of arbitrary composite objects, file I/O, and more. Recent work from our research group tackled the first feature in that list, JVM exception semantics[14]. This paper continues along that path by enabling the acceleration of JVM parallel regions that include object references and dynamic memory allocation.</p> <p>The contributions of this work include 1) serialization and deserialization of JVM objects using a format that is compatible with OpenCL accelerators, 2) advanced code generation techniques for converting JVM bytecode to OpenCL kernels when object references and dynamic memory allocation are used, 3) runtime techniques for supporting dynamic memory allocation on OpenCL accelerators, and 4) a novel redundant data movement elimination technique based on inter-parallel-region dataflow analysis using runtime bytecode inspection.</p> <p>Experimental results presented in this paper show performance improvements of up to 18.33× relative to parallel Java Streams for GPU-accelerated parallel regions, even when those regions include object references and dynamic memory allocation. In our evaluation, we fully characterize where accelerators or the JVM see performance wins and point out opportunities for future work.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Runtime Code Generation and Data Management for Heterogeneous Computing in Java",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807428",
      "doi": "10.1145/2807426.2807428",
      "authors": [
        "Juan José Fumero",
        "Toomas Remmelg",
        "Michel Steuwer",
        "Christophe Dubach"
      ],
      "monthYear": "September 2015",
      "pages": "pp 16–26",
      "shortAbstract": "<p>GPUs (Graphics Processing Unit) and other accelerators are nowadays commonly found in desktop machines, mobile devices and even data centres. While these highly parallel processors offer high raw performance, they also dramatically increase program ...</p><p></p><p></p><p></p>",
      "citations": 13,
      "downloads": 192,
      "id": 85,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>GPUs (Graphics Processing Unit) and other accelerators are nowadays commonly found in desktop machines, mobile devices and even data centres. While these highly parallel processors offer high raw performance, they also dramatically increase program complexity, requiring extra effort from programmers. This results in difficult-to-maintain and non-portable code due to the low-level nature of the languages used to program these devices.</p> <p>This paper presents a high-level parallel programming approach for the popular Java programming language. Our goal is to revitalise the old Java slogan -- Write once, run anywhere --- in the context of modern heterogeneous systems. To enable the use of parallel accelerators from Java we introduce a new API for heterogeneous programming based on array and functional programming. Applications written with our API can then be transparently accelerated on a device such as a GPU using our runtime OpenCL code generator.</p> <p>In order to ensure the highest level of performance, we present data management optimizations. Usually, data has to be translated (marshalled) between the Java representation and the representation accelerators use. This paper shows how marshal affects runtime and present a novel technique in Java to avoid this cost by implementing our own customised array data structure. Our design hides low level data management from the user making our approach applicable even for inexperienced Java programmers.</p> <p>We evaluated our technique using a set of applications from different domains, including mathematical finance and machine learning. We achieve speedups of up to 500× over sequential and multi-threaded Java code when using an external GPU.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Machine-Learning-based Performance Heuristics for Runtime CPU/GPU Selection",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807429",
      "doi": "10.1145/2807426.2807429",
      "authors": [
        "Akihiro Hayashi",
        "Kazuaki Ishizaki",
        "Gita Koblents",
        "Vivek Sarkar"
      ],
      "monthYear": "September 2015",
      "pages": "pp 27–36",
      "shortAbstract": "<p>High-level languages such as Java increase both productivity and portability with productive language features such as managed runtime, type safety, and precise exception semantics. Additionally, Java 8 provides parallel stream APIs with lambda ...</p><p></p><p></p><p></p>",
      "citations": 20,
      "downloads": 377,
      "id": 86,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>High-level languages such as Java increase both productivity and portability with productive language features such as managed runtime, type safety, and precise exception semantics. Additionally, Java 8 provides parallel stream APIs with lambda expressions to facilitate parallel programming for mainstream users of multi-core CPUs and many-core GPUs. These high-level APIs avoid the complexity of writing natively running parallel programs with OpenMP and CUDA/OpenCL through Java Native Interface (JNI). The adoption of such high-level programming models offers opportunities for enabling compilers to perform parallel-aware optimizations and code generation.</p> <p>While many prior approaches have the ability to generate parallel code for both multi-core CPUs and many-core GPUs from Java and other high-level languages, selection of the preferred computing resource between CPUs and GPUs for individual kernels remains one of the most important challenges since a variety of factors affecting performance such as datasets and feature of programs need to be taken into account.</p> <p>This paper explores the possibility of using machine learning to address this challenge. The key idea is to enable a Java runtime to select a preferable hardware device with performance heuristics constructed by supervised machine-learning techniques. For this purpose, if our JIT compiler detects a parallel stream API, 1) our compiler records features of its computation such as the parallel loop range and the number of instructions and 2) our Java runtime generates these features for constructing training data. For the results reported in this paper, we constructed a prediction model with support vector machines (SVMs) after obtaining 291 samples by running 11 applications with different data sets and optimization levels. Our Java runtime then uses the SVMs to make predictions for unseen programs.</p> <p>Our experimental results on an IBM POWER8 platform with NVIDIA Tesla GPUs show that our prediction model predicts a faster configuration with up to 99.0% accuracy with 5-fold cross validation. Based on these results, we conclude that supervised machine-learning is a promising approach for building performance heuristics for mapping Java applications onto accelerators.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "On-Line Synchronous Total Purely Functional Data-Flow Programming on the Java Virtual Machine with Sig",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807430",
      "doi": "10.1145/2807426.2807430",
      "authors": [
        "Baltasar Trancón Widemann",
        "Markus Lepper"
      ],
      "monthYear": "September 2015",
      "pages": "pp 37–50",
      "shortAbstract": "<p>Sig is the prototype of a purely declarative programming language and system for the processing of discrete, clocked synchronous, potentially real-time data streams. It aspires to combine good static safety, scalability and platform independence, with ...</p>",
      "citations": 2,
      "downloads": 55,
      "id": 87,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>Sig is the prototype of a purely declarative programming language and system for the processing of discrete, clocked synchronous, potentially real-time data streams. It aspires to combine good static safety, scalability and platform independence, with semantics that are precise, concise and suitable for domain experts. Its semantical and operational core has been formalized. Here we discuss the general strategy for making Sig programs executable, and describe the current state of a prototype compiler. The compiler is implemented in Java and targets the JVM. By careful cooperation with the JVM just-in-time compiler, it provides immediate executability in a simple and quickly extensible runtime environment, with code performance suitable for moderate real-time applications such as interactive audio synthesis.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Ohua: Implicit Dataflow Programming for Concurrent Systems",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807431",
      "doi": "10.1145/2807426.2807431",
      "authors": [
        "Sebastian Ertel",
        "Christof Fetzer",
        "Pascal Felber"
      ],
      "monthYear": "September 2015",
      "pages": "pp 51–64",
      "shortAbstract": "<p>Concurrent programming has always been a challenging task best left to expert developers. Yet, with the advent of multi-core systems, programs have to explicitly deal with multithreading to fully exploit the parallel processing capabilities of the ...</p>",
      "citations": 2,
      "downloads": 211,
      "id": 88,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>Concurrent programming has always been a challenging task best left to expert developers. Yet, with the advent of multi-core systems, programs have to explicitly deal with multithreading to fully exploit the parallel processing capabilities of the underlying hardware. There has been much research over the last decade on abstractions to develop concurrent code that is both safe and efficient, e.g., using message passing, transactional memory, or event-based programming. In this paper, we focus on the dataflow approach as a way to design scalable concurrent applications. We propose a new dataflow engine and programming framework, named Ohua, that supports implicit parallelism. Ohua marries object-oriented and functional languages: functionality developed in Java can be composed with algorithms in Clojure. This allows us to use different programming styles for the tasks each language is best adapted for. The actual dataflow graphs are automatically derived from the Clojure code. We show that Ohua is not only powerful and easy to use for the programmer, but also produces applications that scale remarkably well: comparative evaluation indicates that a simple web server developed with Ohua outperforms the highly-optimized Jetty server in terms of throughput while being competitive in terms of latency. We also evaluate the impact on energy consumption to validate previous studies indicating that dataflow and message passing can be more energy-efficient than concurrency control based on shared-memory synchronization.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Toward Efficient Strong Memory Model Support for the Java Platform via Hybrid Synchronization",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807432",
      "doi": "10.1145/2807426.2807432",
      "authors": [
        "Aritra Sengupta",
        "Man Cao",
        "Michael D. Bond",
        "Milind Kulkarni"
      ],
      "monthYear": "September 2015",
      "pages": "pp 65–75",
      "shortAbstract": "<p>The Java memory model provides strong behavior guarantees for data-race-free executions. However, it provides very weak guarantees for racy executions, leading to unexpected, unintuitive behaviors. This paper focuses on how to provide a memory model, ...</p><p></p><p></p>",
      "citations": 4,
      "downloads": 56,
      "id": 89,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>The Java memory model provides strong behavior guarantees for data-race-free executions. However, it provides very weak guarantees for racy executions, leading to unexpected, unintuitive behaviors. This paper focuses on how to provide a memory model, called statically bounded region serializability (SBRS), that is substantially stronger than the Java memory model. Our prior work introduces SBRS, as well as compiler and runtime support for enforcing SBRS called EnfoRSer. EnfoRSer modifies the dynamic compiler to insert instrumentation to acquire a lock on each object accessed by the program. For most programs, EnfoRSer's primary run-time cost is executing this instrumentation at essentially every memory access.</p> <p>This paper focuses on reducing the run-time overhead of enforcing SBRS by avoiding instrumentation at every memory access that acquires a per-object lock. We experiment with an alternative approach for providing SBRS that instead acquires a single static lock before each executed region; all regions that potentially race with each other---according to a sound whole-program static analysis--- must acquire the same lock. This approach slows most programs dramatically by needlessly serializing regions that do not actually conflict with each other. We thus introduce a hybrid approach that judiciously combines the two locking strategies, using a cost model and run-time profiling.</p> <p>Our implementation and evaluation in a Java virtual machine use offline profiling and recompilation, thus demonstrating the potential of the approach without incurring online profiling costs. The results show that although the overall performance benefit is modest, our hybrid approach never significantly worsens performance, and for two programs, it significantly outperforms both approaches that each use only one kind of locking. These results demonstrate the potential of a technique based on combining synchronization mechanisms to provide a strong end-to-end memory model for Java and other JVM languages.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Efficient Rebuilding of Large Java Heaps from Event Traces",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807433",
      "doi": "10.1145/2807426.2807433",
      "authors": [
        "Verena Bitto",
        "Philipp Lengauer",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2015",
      "pages": "pp 76–89",
      "shortAbstract": "<p>Understanding and tracking down memory-related performance problems, such as long garbage collection times and memory leaks, is a tedious task in large and complex applications. Memory profilers can support developers in this task by recording detailed ...</p><p></p><p></p>",
      "citations": 12,
      "downloads": 128,
      "id": 90,
      "proceedingsId": 6
    },
    {
      "type": "research-article",
      "title": "Efficient Deterministic Replay of Multithreaded Executions in a Managed Language Virtual Machine",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807434",
      "doi": "10.1145/2807426.2807434",
      "authors": [
        "Michael D. Bond",
        "Milind Kulkarni",
        "Man Cao",
        "Meisam Fathi Salmi",
        "Jipeng Huang"
      ],
      "monthYear": "September 2015",
      "pages": "pp 90–101",
      "shortAbstract": "<p>Shared-memory parallel programs are inherently nondeterministic, making it difficult to diagnose rare bugs and to achieve deterministic execution. Existing multithreaded record &amp; replay approaches have serious limitations such as relying on custom ...</p><p></p>",
      "citations": 11,
      "downloads": 131,
      "id": 91,
      "proceedingsId": 6
    },
    {
      "type": "research-article",
      "title": "An Efficient and Generic Event-based Profiler Framework for Dynamic Languages",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807435",
      "doi": "10.1145/2807426.2807435",
      "authors": [
        "Gülfem Savrun-Yeniçeri",
        "Michael L. Van de Vanter",
        "Per Larsen",
        "Stefan Brunthaler",
        "Michael Franz"
      ],
      "monthYear": "September 2015",
      "pages": "pp 102–112",
      "shortAbstract": "<p>Profilers help programmers analyze their programs and identify performance bottlenecks. We implement a profiler framework that helps to compare and analyze programs implementing the same algorithms written in different languages. Profiler implementers ...</p>",
      "citations": 5,
      "downloads": 208,
      "id": 92,
      "proceedingsId": 6
    },
    {
      "type": "research-article",
      "title": "Improving the Interoperation between Generics Translations",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807436",
      "doi": "10.1145/2807426.2807436",
      "authors": [
        "Vlad Ureche",
        "Milos Stojanovic",
        "Romain Beguet",
        "Nicolas Stucki",
        "Martin Odersky"
      ],
      "monthYear": "September 2015",
      "pages": "pp 113–124",
      "shortAbstract": "<p>Generics on the Java platform are compiled using the erasure transformation, which only supports by-reference values. This causes slowdowns when generics operate on primitive types, such as integers, as they have to be transformed into reference-based ...</p><p></p><p></p><p></p>",
      "citations": 3,
      "downloads": 85,
      "id": 93,
      "proceedingsId": 6
    },
    {
      "type": "research-article",
      "title": "Identifying Test Refactoring Candidates with Assertion Fingerprints",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807437",
      "doi": "10.1145/2807426.2807437",
      "authors": [
        "Zheng Felix Fang",
        "Patrick Lam"
      ],
      "monthYear": "September 2015",
      "pages": "pp 125–137",
      "shortAbstract": "<p>Test cases constitute around 30% of the codebase of a number of large software systems. Poor design of test suites hinders test comprehension and maintenance. Developers often copy-paste existing tests and reproduce both boilerplate and essential ...</p><p></p><p></p>",
      "citations": 1,
      "downloads": 123,
      "id": 94,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>Test cases constitute around 30% of the codebase of a number of large software systems. Poor design of test suites hinders test comprehension and maintenance. Developers often copy-paste existing tests and reproduce both boilerplate and essential environment setup code as well as assertions. Test case refactoring would be valuable for developers aiming to control technical debt arising due to copy-pasted test cases.</p> <p>In the context of test code, identifying candidates for refactoring requires tedious manual effort. In this work, we specifically tailor static analysis techniques for test analysis. We present a novel technique, assertion fingerprints, for finding similar test cases based on the set of assertion calls in test methods. Assertion fingerprints encode the control flow around the ordered set of assertions in methods.</p> <p>We have implemented similar test case detection using assertion fingerprints and applied it to 10 test suites for open-source Java programs. We provide an empirical study and a qualitative analysis of our results. Assertion fingerprints enable the discovery of tests that exhibit strong structural similarities and are amenable to refactoring. Our technique delivers an overall 75% true positive rate on our benchmarks and reports that 40% of the benchmark test methods are potentially refactorable.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Incremental Co-Evolution of Java Programs based on Bidirectional Graph Transformation",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807438",
      "doi": "10.1145/2807426.2807438",
      "authors": [
        "Sven Peldszus",
        "Géza Kulcsár",
        "Malte Lochau",
        "Sandro Schulze"
      ],
      "monthYear": "September 2015",
      "pages": "pp 138–151",
      "shortAbstract": "<p>Modern Java IDE aim at assisting object-oriented software development workflows with continuously interleaved co-evolution steps of program editing and program refactoring. Program editing usually comprises manually performed program changes applied by ...</p>",
      "citations": 3,
      "downloads": 137,
      "id": 95,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>Modern Java IDE aim at assisting object-oriented software development workflows with continuously interleaved co-evolution steps of program editing and program refactoring. Program editing usually comprises manually performed program changes applied by a programmer at source code level. In contrast, refactorings consist of behavior-preserving program restructuring rules with complex preconditions, usually formulated over an appropriate program abstraction. To integrate both steps into a comprehensive program evolution framework, we present a graph-based approach for incremental co-evolution of Java programs. Our approach is based on a concise graph-based representation of Java programs by means of a reduced abstract syntax tree, augmented with additional cross-tree edges denoting crucial semantic information. On this basis, a precise formal specification of object-oriented program refactorings can be defined in terms of endogenous graph-transformation rules. In addition, we use Triple Graph Grammars (TGG) to define exogenous bidirectional graph transformation rules for automated incremental synchronization between a program graph and the corresponding source code. Our implementation relies on the graph-transformation engine eMoflon and currently supports the Java refactorings Pull Up Method and Create Superclass.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "BLAST: Bytecode-Level Analysis on Sliced Traces",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807439",
      "doi": "10.1145/2807426.2807439",
      "authors": [
        "Mohammad R. Azadmanesh",
        "Matthias Hauswirth"
      ],
      "monthYear": "September 2015",
      "pages": "pp 152–158",
      "shortAbstract": "<p>BLAST is a framework for deep analysis of Java program executions. It captures the entire information flow of an execution. A BLAST trace represents the space-time history of a program run: It combines space (heap, static, and stack memory locations) ...</p>",
      "citations": 5,
      "downloads": 195,
      "id": 96,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>BLAST is a framework for deep analysis of Java program executions. It captures the entire information flow of an execution. A BLAST trace represents the space-time history of a program run: It combines space (heap, static, and stack memory locations) and time (executed bytecode instructions). The BLAST API allows to easily slice and navigate that history along the data and control dependencies of the execution. This paper introduces BLAST and shows one possible application: root cause analysis for debugging unit test failures.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Mining Method Handle Graphs for Efficient Dynamic JVM Languages",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807440",
      "doi": "10.1145/2807426.2807440",
      "authors": [
        "Shijie Xu",
        "David Bremner",
        "Daniel Heidinga"
      ],
      "monthYear": "September 2015",
      "pages": "pp 159–169",
      "shortAbstract": "<p>The Java Virtual Machine (JVM) has been used as an execution platform by many dynamically-typed programming languages such as Ruby, Python, and Groovy. The main challenge to compile such dynamic JVM languages is choosing the most appropriate ...</p><p></p>",
      "citations": 2,
      "downloads": 94,
      "id": 97,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>The Java Virtual Machine (JVM) has been used as an execution platform by many dynamically-typed programming languages such as Ruby, Python, and Groovy. The main challenge to compile such dynamic JVM languages is choosing the most appropriate implementation of a method for various types of an object at runtime. To address this challenge, a new Java bytecode instruction, invokedynamic, has been introduced, allowing users to control the linkage between a call site and a method implementation. With this instruction, a method handle that refers to a method is linked to the call site and then potentially transforms the invocation to a real implementation. As a referenced method of a method handle might in turn refer to other method handles, multiple method handles constitute a Method Handle Graph (MHG).</p> <p>In order to support more efficient dynamic JVM language implementations, we present methods to mine patterns in the method handle graph. We investigate two kinds of method handle patterns: the transformation pattern and the instance pattern. The transformation pattern refers to a composition of multiple method handle transformations, and the instance pattern refers to equivalent method handles in MHGs. Both patterns are mined by the presented suffix tree and equivalency detector, respectively, which are implemented as modules in the Method Handle Mining System (MHMS). Our experiments on the JRuby Micro-Indy benchmark reveal several findings: a) the frequency of different transformation patterns varies significantly, and the JRuby interpreter prefers a small number of transformation patterns, b) a large proportion of method handles, 28.83%, are equivalent, and most of these equivalent method handles can be eliminated to reduce consumed memory, and c) the distribution of equivalent sets for length-two method handle chains is also uneven. For example, only 7% of these sets have more than 30 equivalent method handle chains. We believe these insights are important steps towards further optimizations based on method handle graphs.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "SmartStealing: Analysis and Optimization of Work Stealing in Parallel Garbage Collection for Java VM",
      "url": "https://dl.acm.org/doi/10.1145/2807426.2807441",
      "doi": "10.1145/2807426.2807441",
      "authors": [
        "Junjie Qian",
        "Witawas Srisa-an",
        "Du Li",
        "Hong Jiang",
        "Sharad Seth",
        "Yaodong Yang"
      ],
      "monthYear": "September 2015",
      "pages": "pp 170–181",
      "shortAbstract": "<p>Parallel garbage collection has been used to speedup the collection process on multicore architectures. Similar to other parallel techniques, balancing the workload among threads is critical to ensuring good overall collection performance. To this end, ...</p><p></p>",
      "citations": 5,
      "downloads": 190,
      "id": 98,
      "proceedingsId": 6,
      "fullAbstract": "\n\t\t<p>Parallel garbage collection has been used to speedup the collection process on multicore architectures. Similar to other parallel techniques, balancing the workload among threads is critical to ensuring good overall collection performance. To this end, work stealing is employed by the current state-of-the-art Java Virtual Machine, OpenJDK, to keep GC threads from idling during a collection process. However, we found that the current algorithm is not efficient. Its usage can often cause GC performance to be worse than when work stealing is not used.</p> <p>In this paper, we identify three factors that affect work stealing efficiency: determining tasks that can benefit from stealing, frequency with which to attempt stealing, and performance impacts of failed stealing attempts. Based on this analysis, we propose SmartStealing, a new algorithm that can automatically decide whether to attempt stealing at a particular point during execution. If stealing is attempted, it can efficiently identify a task to steal from. We then compare the collection performances when (i) the default work stealing algorithm is used, (ii) work stealing is not used at all, and (iii) the SmartStealing approach is used. Without modifying the remaining garbage collection system, the evaluation result shows that SmartStealing can reduce the parallel GC execution time for 19 of the 21 benchmarks. The average reduction is 50.4% and the highest reduction is 78.7%. We also investigate the performances of SmartStealing on NUMA and UMA architectures.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Implementing agent roles in Java",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2655184",
      "doi": "10.1145/2647508.2655184",
      "authors": [
        "Giacomo Cabri"
      ],
      "monthYear": "September 2014",
      "pages": "pp 1–3",
      "shortAbstract": "<p>Roles represent a powerful means to enable software agents to act in open environments. They can be implemented in different ways, and in this talk I will show two directions exploiting Java. The former one is quite traditional and exploits composition; ...</p>",
      "citations": 0,
      "downloads": 125,
      "id": 99,
      "proceedingsId": 7,
      "fullAbstract": "\n\t\t<p>Roles represent a powerful means to enable software agents to act in open environments. They can be implemented in different ways, and in this talk I will show two directions exploiting Java. The former one is quite traditional and exploits composition; the latter one adds the capabilities of roles to agents' classes in form of injected bytecode. I will compare the two approaches trying to generalize the considerations.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Comprehensive multi-platform dynamic program analysis for the Java and Dalvik virtual machines",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2655186",
      "doi": "10.1145/2647508.2655186",
      "authors": [
        "Yudi Zheng",
        "Haiyang Sun",
        "Lubomir Bulej",
        "Petr Tůma",
        "Walter Binder"
      ],
      "monthYear": "September 2014",
      "pages": "pp 4",
      "shortAbstract": "<p>Despite its importance for many software engineering tasks, dynamic program analysis is only insufficiently supported on the Java platform [2]. Existing Java Virtual Machines (JVMs) as well as Android's Dalvik Virtual Machine (DVM) lack dedicated ...</p><p></p>",
      "citations": 0,
      "downloads": 155,
      "id": 100,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Generic traits for the Java platform",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647518",
      "doi": "10.1145/2647508.2647518",
      "authors": [
        "Lorenzo Bettini",
        "Ferruccio Damiani"
      ],
      "monthYear": "September 2014",
      "pages": "pp 5–16",
      "shortAbstract": "<p>A trait is a set of methods that is independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. Traits were proposed as a mechanism for fine-grained code reuse to overcome ...</p>",
      "citations": 3,
      "downloads": 208,
      "id": 101,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "TruffleC: dynamic execution of C on a Java virtual machine",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647528",
      "doi": "10.1145/2647508.2647528",
      "authors": [
        "Matthias Grimmer",
        "Manuel Rigger",
        "Roland Schatz",
        "Lukas Stadler",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2014",
      "pages": "pp 17–26",
      "shortAbstract": "<p>This paper presents TruffleC, a C interpreter that allows the dynamic execution of C code on top of a Java Virtual Machine (JVM). Rather than producing a static build of a C application, TruffleC is a self-optimizing abstract syntax tree (AST) ...</p><p></p><p><em></em></p>",
      "citations": 15,
      "downloads": 487,
      "id": 102,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Instance pointcuts: selecting object sets based on their usage history",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647526",
      "doi": "10.1145/2647508.2647526",
      "authors": [
        "Christoph Bockisch",
        "Kardelen Hatun",
        "Mehmet Aksit"
      ],
      "monthYear": "September 2014",
      "pages": "pp 27–38",
      "shortAbstract": "<p>At runtime, how objects have to be handled frequently depends on how they were used before. But with current programming-language support, selecting objects according to their previous usage patterns often results in scattered and tangled code. In this ...<em></em></p>",
      "citations": 0,
      "downloads": 54,
      "id": 103,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Fork/join parallelism in the wild: documenting patterns and anti-patterns in Java programs using the fork/join framework",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647511",
      "doi": "10.1145/2647508.2647511",
      "authors": [
        "Mattias De Wael",
        "Stefan Marr",
        "Tom Van Cutsem"
      ],
      "monthYear": "September 2014",
      "pages": "pp 39–50",
      "shortAbstract": "<p>Now that multicore processors are commonplace, developing parallel software has escaped the confines of high-performance computing and enters the mainstream. The Fork/Join framework, for instance, is part of the standard Java platform since version 7. ...</p><p><em></em></p>",
      "citations": 12,
      "downloads": 197,
      "id": 104,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Efficient code management for dynamic multi-tiered compilation systems",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647513",
      "doi": "10.1145/2647508.2647513",
      "authors": [
        "Tobias Hartmann",
        "Albert Noll",
        "Thomas Gross"
      ],
      "monthYear": "September 2014",
      "pages": "pp 51–62",
      "shortAbstract": "<p>Managed runtime environments (MREs) like web browsers or the Java Virtual Machine (JVM) include multiple dynamic compilers (or compiler configurations) to provide a good tradeoff between compilation speed and generated code quality: Longer compilation ...<em></em></p><p></p><p></p>",
      "citations": 3,
      "downloads": 149,
      "id": 105,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "DeltaJ 1.5: delta-oriented programming for Java 1.5",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647512",
      "doi": "10.1145/2647508.2647512",
      "authors": [
        "Jonathan Koscielny",
        "Sönke Holthusen",
        "Ina Schaefer",
        "Sandro Schulze",
        "Lorenzo Bettini",
        "Ferruccio Damiani"
      ],
      "monthYear": "September 2014",
      "pages": "pp 63–74",
      "shortAbstract": "<p>Delta-oriented programming (DOP) is a modular, yet flexible approach to implement software product lines. In DOP, a product line is implemented by a set of deltas, which are containers of modifications to a program. A delta-oriented product line is ...</p>",
      "citations": 42,
      "downloads": 191,
      "id": 106,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Habanero-Java library: a Java 8 framework for multicore programming",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647514",
      "doi": "10.1145/2647508.2647514",
      "authors": [
        "Shams Imam",
        "Vivek Sarkar"
      ],
      "monthYear": "September 2014",
      "pages": "pp 75–86",
      "shortAbstract": "<p>With the advent of the multicore era, it is clear that future growth in application performance will primarily come from increased parallelism. We believe parallelism should be introduced early into the Computer Science curriculum to educate students on ...</p><p></p><p></p>",
      "citations": 62,
      "downloads": 342,
      "id": 107,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Side effect monitoring for Java using bytecode rewriting",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647515",
      "doi": "10.1145/2647508.2647515",
      "authors": [
        "Manuel Geffken",
        "Peter Thiemann"
      ],
      "monthYear": "September 2014",
      "pages": "pp 87–98",
      "shortAbstract": "<p>A side effect of a method in Java is a read or write operation that the method may perform on an object in the heap. Methods with side effects are more difficult to understand and to reason about than pure methods, in particular in the presence of ...</p><p></p><p></p>",
      "citations": 0,
      "downloads": 85,
      "id": 108,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "CFL-reachability and context-sensitive integrity types",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647522",
      "doi": "10.1145/2647508.2647522",
      "authors": [
        "Ana Milanova",
        "Wei Huang",
        "Yao Dong"
      ],
      "monthYear": "September 2014",
      "pages": "pp 99–109",
      "shortAbstract": "<p>Integrity types can help detect information flow vulnerabilities in web applications and Android apps. We study DFlow, a context-sensitive integrity type system and we give an interpretation of DFlow in terms of CFL-reachability. We propose DFlowCFL, a ...</p>",
      "citations": 5,
      "downloads": 67,
      "id": 109,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Selective regression testing for web applications created with google web toolkit",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647527",
      "doi": "10.1145/2647508.2647527",
      "authors": [
        "Matthias Hirzel"
      ],
      "monthYear": "September 2014",
      "pages": "pp 110–121",
      "shortAbstract": "<p>Today's web applications are highly dynamic and powerful software components that may change often. Mostly, they are based on JavaScript or AJAX. A common way to ensure correct behaviour is to use selective regression tests. Nevertheless, especially on ...</p>",
      "citations": 2,
      "downloads": 177,
      "id": 110,
      "proceedingsId": 7,
      "fullAbstract": "\n\t\t<p>Today's web applications are highly dynamic and powerful software components that may change often. Mostly, they are based on JavaScript or AJAX. A common way to ensure correct behaviour is to use selective regression tests. Nevertheless, especially on the client side, testing is hard. One way to ease the development and the testing process of dynamic web applications is to use the Google Web Toolkit (GWT). This framework enables the development in Java and transfers the code via a compiler into JavaScript. However, it does not support regression testing with test selection. As far as we know, this paper presents the first selective regression testing technique for GWT-based web applications. In order to determine test cases that have to be rerun, it compares the Java code of two versions of the application, localizes and classifies changes in the code, and traces the mapping of Java source code to JavaScript code. We have implemented our technique as a prototype Eclipse plug-in and have conducted an evaluation of the tool.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Applying source level auto-vectorization to Aparapi Java",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647519",
      "doi": "10.1145/2647508.2647519",
      "authors": [
        "Curt Albert",
        "Alastair Murray",
        "Binoy Ravindran"
      ],
      "monthYear": "September 2014",
      "pages": "pp 122–132",
      "shortAbstract": "<p>Parallelism dominates modern hardware design, from multi-core CPUs to SIMD and GPGPU. This bring with it, however, a need to program this hardware in a programmer-friendly manner. Traditionally, managed languages like Java have struggled to take ...</p><p></p><p></p>",
      "citations": 1,
      "downloads": 135,
      "id": 111,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "An object storage model for the truffle language implementation framework",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647517",
      "doi": "10.1145/2647508.2647517",
      "authors": [
        "Andreas Wöß",
        "Christian Wirth",
        "Daniele Bonetta",
        "Chris Seaton",
        "Christian Humer",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2014",
      "pages": "pp 133–144",
      "shortAbstract": "<p>Truffle is a Java-based framework for developing high-performance language runtimes. Language implementers aiming at developing new runtimes have to design all the runtime mechanisms for managing dynamically typed objects from scratch. This not only ...</p><p></p>",
      "citations": 37,
      "downloads": 386,
      "id": 112,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Fast Java profiling with scheduling-aware stack fragment sampling and asynchronous analysis",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647509",
      "doi": "10.1145/2647508.2647509",
      "authors": [
        "Peter Hofer",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2014",
      "pages": "pp 145–156",
      "shortAbstract": "<p>Sampling is a popular approach to profiling because it typically has only a small impact on performance and does not modify the profiled application. Common sampling profilers collect data about an application by pausing the application threads, walking ...<em></em></p><p><em></em></p>",
      "citations": 6,
      "downloads": 157,
      "id": 113,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Comparing points-to static analysis with runtime recorded profiling data",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647524",
      "doi": "10.1145/2647508.2647524",
      "authors": [
        "Codruţ Stancu",
        "Christian Wimmer",
        "Stefan Brunthaler",
        "Per Larsen",
        "Michael Franz"
      ],
      "monthYear": "September 2014",
      "pages": "pp 157–168",
      "shortAbstract": "<p>We present an empirical study that sheds new light on static analysis results precision by comparing them with runtime collected data. Our motivation is finding additional sources of information that can guide static analysis for increased application ...</p><p></p>",
      "citations": 3,
      "downloads": 193,
      "id": 114,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Closed and open nested atomic actions for Java: language design and prototype implementation",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647525",
      "doi": "10.1145/2647508.2647525",
      "authors": [
        "Keith Chapman",
        "Antony L. Hosking",
        "J. Eliot B. Moss",
        "Tim Richards"
      ],
      "monthYear": "September 2014",
      "pages": "pp 169–180",
      "shortAbstract": "<p>We describe the design and prototype implementation of a dialect of Java, XJ, that supports both closed and open nested transactions. As we have previously advocated, open nesting most naturally attaches to the <em>class</em> as the primary abstraction mechanism ...</p>",
      "citations": 4,
      "downloads": 104,
      "id": 115,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Trait-oriented programming in Java 8",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647520",
      "doi": "10.1145/2647508.2647520",
      "authors": [
        "Viviana Bono",
        "Enrico Mensa",
        "Marco Naddeo"
      ],
      "monthYear": "September 2014",
      "pages": "pp 181–186",
      "shortAbstract": "<p>Java 8 was released recently. Along with lambda expressions, a new language construct is introduced: default methods in interfaces. The intent of this feature is to allow interfaces to be extended over time preserving backward compatibility. In this ...</p>",
      "citations": 8,
      "downloads": 245,
      "id": 116,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Speculation without regret: reducing deoptimization meta-data in the Graal compiler",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647521",
      "doi": "10.1145/2647508.2647521",
      "authors": [
        "Gilles Duboscq",
        "Thomas Würthinger",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2014",
      "pages": "pp 187–193",
      "shortAbstract": "<p>Speculative optimizations are used in most Just In Time (JIT) compilers in order to take advantage of dynamic runtime feedback. These speculative optimizations usually require the compiler to produce meta-data that the Virtual Machine (VM) can use as ...<em></em></p>",
      "citations": 15,
      "downloads": 251,
      "id": 117,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "JNIF: Java native instrumentation framework",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647516",
      "doi": "10.1145/2647508.2647516",
      "authors": [
        "Luis Mastrangelo",
        "Matthias Hauswirth"
      ],
      "monthYear": "September 2014",
      "pages": "pp 194–199",
      "shortAbstract": "<p>The development of instrumentation-based dynamic analyses for Java bytecode is enabled by various bytecode rewriting frameworks. Those frameworks are all implemented in Java. This complicates their use for developing <em>full-coverage</em> analyses that not only ...<em></em></p><p></p>",
      "citations": 2,
      "downloads": 279,
      "id": 118,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Cooperari: a tool for cooperative testing of multithreaded Java programs",
      "url": "https://dl.acm.org/doi/10.1145/2647508.2647523",
      "doi": "10.1145/2647508.2647523",
      "authors": [
        "Eduardo R. B. Marques",
        "Francisco Martins",
        "Miguel Simões"
      ],
      "monthYear": "September 2014",
      "pages": "pp 200–206",
      "shortAbstract": "<p>Bugs in multithreaded application can be elusive. They are often hard to trace and replicate, given the usual non-determinism and irreproducibility of scheduling decisions at runtime. We present Cooperari, a tool for deterministic testing of ...</p>",
      "citations": 2,
      "downloads": 100,
      "id": 119,
      "proceedingsId": 7
    },
    {
      "type": "research-article",
      "title": "Deriving code coverage information from profiling data recorded for a trace-based just-in-time compiler",
      "url": "https://dl.acm.org/doi/10.1145/2500828.2500829",
      "doi": "10.1145/2500828.2500829",
      "authors": [
        "Christian Häubl",
        "Christian Wimmer",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2013",
      "pages": "pp 1–12",
      "shortAbstract": "<p>Code coverage information is typically recorded by adding instrumentation to an application before or during execution. However, this has the disadvantage that the instrumentation decreases the performance.</p> <p>A high-performance virtual machine (VM), such ...</p><p></p>",
      "citations": 2,
      "downloads": 466,
      "id": 120,
      "proceedingsId": 8
    },
    {
      "type": "research-article",
      "title": "TAE-JS: automated enhancement of JavaScript programs by leveraging the Java annotations infrastructure",
      "url": "https://dl.acm.org/doi/10.1145/2500828.2500830",
      "doi": "10.1145/2500828.2500830",
      "authors": [
        "Myoungkyu Song",
        "Eli Tilevich"
      ],
      "monthYear": "September 2013",
      "pages": "pp 13–24",
      "shortAbstract": "<p>Recent state-of-the-art approaches enhance JavaScript programs with concerns (e.g., persistence, security, transactions, etc.) by modifying the source code by hand to use special libraries. As a result, adding concerns to a JavaScript program creates ...</p>",
      "citations": 1,
      "downloads": 108,
      "id": 121,
      "proceedingsId": 8,
      "fullAbstract": "\n\t\t<p>Recent state-of-the-art approaches enhance JavaScript programs with concerns (e.g., persistence, security, transactions, etc.) by modifying the source code by hand to use special libraries. As a result, adding concerns to a JavaScript program creates divergent codebases that must be maintained separately. At the core of the problem is that JavaScript lacks metadata to express concerns declaratively. In this paper, we present a declarative approach to enhancing JavaScript programs that applies the Java annotations infrastructure to JavaScript, without extending the JavaScript language. An IDE combines JavaScript and Java during the development, but processes the languages separately. Programmers declare how concerns should be added to a JavaScript program using Java annotations. Based on the annotations, a code generator synthesizes aspect code that adds the specified concerns. Although these enhancements are implemented as third-party libraries, our approach can transparently insert them into JavaScript programs given a declarative specification.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "Cross-compiling Java to JavaScript via tool-chaining",
      "url": "https://dl.acm.org/doi/10.1145/2500828.2500831",
      "doi": "10.1145/2500828.2500831",
      "authors": [
        "Arno Puder",
        "Victor Woeltjen",
        "Alon Zakai"
      ],
      "monthYear": "September 2013",
      "pages": "pp 25–34",
      "shortAbstract": "<p>Cross-compilation translates between different high-level programming languages and enables programmers to use their skill-set on a platform that ordinarily does not support their preferred language. E.g., web applications for desktop browsers and ...</p>",
      "citations": 3,
      "downloads": 401,
      "id": 122,
      "proceedingsId": 8,
      "fullAbstract": "\n\t\t<p>Cross-compilation translates between different high-level programming languages and enables programmers to use their skill-set on a platform that ordinarily does not support their preferred language. E.g., web applications for desktop browsers and mobile devices such as Firefox OS or Tizen need to be written in JavaScript and are not easily accessible to Java developers. The combinatoric explosion of possible cross-compilers between languages leads to the idea of tool-chaining: instead of a dedicated cross-compiler, chain two already existing cross-compilers. In this paper we demonstrate the chaining of two cross-compilers -- XMLVM and Emscripten -- to translate first from Java to C and then from C to JavaScript. We describe minor adjustments to XMLVM and Emscripten to optimize the generated code resulting in a competitive cross-compiler. Benchmarks show that tool-chaining yields similar performance relative to dedicated cross-compilers. Specifically, we will compare XMLVM/Emscripten with Google's GWT.</p>\n\t"
    },
    {
      "type": "research-article",
      "title": "An efficient native function interface for Java",
      "url": "https://dl.acm.org/doi/10.1145/2500828.2500832",
      "doi": "10.1145/2500828.2500832",
      "authors": [
        "Matthias Grimmer",
        "Manuel Rigger",
        "Lukas Stadler",
        "Roland Schatz",
        "Hanspeter Mössenböck"
      ],
      "monthYear": "September 2013",
      "pages": "pp 35–44",
      "shortAbstract": "<p>We present an efficient and dynamic approach for calling native functions from within Java. Traditionally, programmers use the <em>Java Native Interface</em> (JNI) to call such functions. This paper introduces a new mechanism which we tailored specifically ...<em></em><em></em></p><p><em></em><em></em></p><p><em></em><em></em><em></em></p>",
      "citations": 11,
      "downloads": 551,
      "id": 123,
      "proceedingsId": 8
    },
    {
      "type": "research-article",
      "title": "OCaml-Java: an ML implementation for the Java ecosystem",
      "url": "https://dl.acm.org/doi/10.1145/2500828.2500833",
      "doi": "10.1145/2500828.2500833",
      "authors": [
        "Xavier Clerc"
      ],
      "monthYear": "September 2013",
      "pages": "pp 45–56",
      "shortAbstract": "<p>Developing complex applications often requires to mix several languages, in order to take advantage of each language strengths in the various parts of the source code. However, this imposes to transfer data from one language to another one; the problem ...</p><p></p><p></p>",
      "citations": 2,
      "downloads": 401,
      "id": 124,
      "proceedingsId": 8
    }
  ]
}
